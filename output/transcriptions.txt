===== BEGIN TRANSCRIPT =====
Video ID: Y0vpnE6FyGs
Language: en
Source: auto-generated
Saved At: 2025-08-15T19:16:28.640171
It happened on August 5th. Openai released a new model, but sadly it was not GPT5 as it was rumored before. So, we're going to see GPT5 later in August. Maybe even in the next days. There's another rumor that it's going to be released on 8th of August. But we will have to see what is really happening. Despite that, uh, OpenI released another very interesting model a lot of people have been waiting for. And then also directly uh Antropic yeah they released also a new model a small upgrade to the previous ones and we want to talk about this now in uh this video. So first of all openai released their openw weight models called GPD OSS and there is 120 billion parameter models as well as 20 billion uh parameter models. Yeah, they are open weight, not completely open source, but they really should be able to uh compete with other um projects from metal, FLA for example, Mistral or especially also all the Chinese open-source model which are coming on the market and also companies are building based on that and so yeah open AI pivot uh this is the first openweight model since GPT2 so that's actually quite a long time and we know there were yeah many disputes boots in the last year from the company if it should go in a closed um yeah more closed environment and company structure than it was and in the beginning from the core on uh meant to be and then at the same time we saw a new upgrade to the claude ous family so claude oppus 4.1 uh it's a minor upgrade but we're going to talk about that more uh a bit later so this strategy from opening I to yeah release an open weight model is uh yeah long overdue I would say. Um so they give a lot of uh benefits with this model and actually also the performance should be on par with other top tier models which are out now also from uh for example open AAI but they put a lot of uh also investigation security measurements that the model is not too intelligent so it could cause even with finetuning and a lot of compute harm to humanity let's say yeah in biological way in cyber security way and so on um but I think it's very important to keep and stay competitive for open eye to have this open weight models. It's not completely open source. We talk a bit later more about this to really stay competitive and uh keep up to date with the demand uh from certain governments, regulatory uh parties and so on. So these new GPTOs models they are based on the mixture of experts technology or architecture. So that means while they be used they don't use the full 120 billion or 20 billion at once. No they only use a fraction of these. Uh in the case of the GPOSS 120 billion parameters we use only 5.1 billion while being active and the other one uses just 3.6 and for that that means also you need le less compute. It's way more efficient which is great to see. And so basically the 120 billion parameter uh model is dedicated for uh or targeted at hardware like GPU clusters from or single GPU cluster from Nvidia for example the H100 but still this one cost several tous thousands of dollars while this smaller model can be running locally maybe even on a more less standard PC with a VRAM of higher than 16 GB. bites. So that's I think a big benefit for a lot of people uh who want to run applications maybe in their companies um but they don't have uh so many resources in terms of money or skill sets. So they are way easier to run there. So when we talk about the benchmarks of these models, we're going to see that GPD uh the open-source or not open source open weight model basically has quite good performance also comparing to other models like 03 04 mini with tool use. Uh we can see uh it's almost on par like with 03 while the smaller model can also with without tool use uh be even smarter than 03 mini was without tools. Yeah. in the code force benchmark. So then in the humanities last exam uh test we have uh yeah still all three with tools like way smarter than uh for example uh TBT OSS with the 120 billion parameters with tools. So there are still differences in the health bench heart uh benchmark. We have quite competitive 30 points uh compared to the 70.5 points from the O4 mini which is in my opinion quite interesting and a very smart uh or very good 10.8 percentage score compared to the 03 mini with four. Yeah. And at the IME 25 benchmark with tools, we have uh GPDOSS with 97.9% accuracy compared over to 04's uh mini 99.5. So there's still some way to go, but it's still definitely a quite interesting model. At the MMLU benchmark, we have the 120 billion parameter model with a result of 90% accuracy. Again compared to the O4 mini a bit below cuz this one is getting 93% uh again here the 20 billion parameter model can com quite compete with the 87 uh% accuracy of the 03 mini model. So it's kind of in between. Yeah. But still in most cases not above the uh 03 model. Um for example also here in the towel bench uh benchmark. Yeah. And despite the facts that these were comparisons between yeah fully closed models compared to an open weight model we have to also make differences between like open weight models like the GPT OSS and for example a deepseek model which is fully open source. So the distinction is made that also here G uh OpenAI is not sharing the information or the data where the models have been trained on. So they keep this for them as a secret and uh do can give out uh the most important things to fine-tune models uh to developers or to governments who can use on this model and they will have models made out of US and not for example from China. So, Entropic on the other side, um, yeah, they launched Claude Opus 4.1. It's, uh, at the same time almost, yeah, it's just not a big upgrade. It's really a minor upgrade to the latest model. So, some of the key improvements from benchmarks when we look here at SWE benchmark is uh that it got a bit smarter. It doesn't seem like a big step from 72.5% to 74.5% but these small marginal uh gains can mean uh despite that uh still some positive impact which cloth or entropic is also showing through some testimonials. Um but when we look here at the benchmarks we see that um yeah sonnet in comparison to sonnet the new 4.1 uh cla oppos reached 74.5 uh in the graduate label reasoning uh we have 80.9 comparison to the 79.6 versus 75.4 still here Gemini Pro is with its 86.4 before uh quite higher. Uh we have also some new um benchmarks for visual reasoning 77.1% versus for example 74.4% from the set uh or the OPUS 4 model with 76.5%. Yeah. uh but still in some areas Gemini is still more advanced but we know that CLA is really specializing their models for example for um yeah coding and there they got uh some testimonials for example from GitHub where GitHub noted that it's particular notable performance gains in multifile code refactoring and also Rakutin group they praise the model's ability to pinpoint exact correct actions within large code bases without making unnecessary adjustments or introducing bugs. So we have to see in real world what that means. Um but I would say when GPT5 will come out the score in these benchmarks will be way higher with GPT and then we have to see what entrepre Antropic will be countering with. Yeah. But still there was some important other news during uh the announcement of cloud opus 4 cause uh now entropic and claude got directly integrated into GitHub's copilot for enterprise and pro plus users. So uh they are emboding their best model directly into the software development cycle which is used by millions of developers um through GitHub. Um that's quite a powerful move and we have to see like how it plays out. So yeah, that was quite an interesting day. Sadly not what we hoped for that we finally see finally see GPD5 but it was still interesting and I think also GPD OSS is a quite important move and we have to see how smart it is and uh in which applications or for example which governance will jump on that train and using these open weight models from uh open AI and also interesting to see that clot on the same day uh released the OPUS 4.1 uh model. But that's how it is going in AI. Yeah, the race is never over. And in a few days, we're going to see GPT5 and we are going to talk about it. We're going to test it. We will build applications with uh GPT5 inside. And if you want to see that, then you should make sure that you follow my channel and I will help you to benefit from this AI race. Thank you for watching and see you soon.

===== END TRANSCRIPT =====
===== BEGIN TRANSCRIPT =====
Video ID: SqtF-D4tPuI
Language: en
Source: auto-generated
Saved At: 2025-08-15T19:16:29.457498
OpenAI has finally released open models. They have two open weight models. They're both reasoning models. I'm running it right now on my laptop. One of them right now on my laptop. I'm going to show you how to install it locally. It's going to be very, very easy, non-technical. You can install and run it right now. And I'll put a link in the description to this page, too. So, you'll see the two open models here. And as usual, the naming is kind of weird, but GPTOSs 12B. This is a bigger model and currently it doesn't run really well on my laptop, but I do have an Nvidia PC that runs a really beefy GPU. So, I'm going to do a different video running this one locally on that computer. Right now, I have this one running on my computer, GPTOSs 20B. This is the mediumsiz open model that runs on local desktop and laptop. So, this is going to require a high-end desktop and laptop. And just so I could show you what I'm running this on right now, this is my current laptop that I'm running this on. So, Apple M3 Max, 64 gigs of RAM, and I have plenty of storage here. So, the size was actually in like 12, 13 GB, so no issues there. So, again, this is a pretty high-end laptop, but 120 is still kind of out of the range. This is 120 billion parameter. This is 20 billion parameter. But let me show you how to install it. So if you click right here to start building, it's going to take you to hugging phase. So if you're more technical, you can download it this way, but I usually try to show things that are a lot less technical. So let's try to figure out how to do this in a lot less technical way. On the same page, they have this interactive mode where you could go ahead and try this on the web. This is at gptosss.com. And you could try it here. So you could just type in a prompt and it will go to work. It's probably going to be slow right now because it just came out. So, a lot of people are going to use it locally, it's a whole different story. It's actually really fast. So, to run it locally, and by the way, when you first run it, it's going to ask you if you want to show the reasoning or hide the reasoning. So, you could do that with this checkbox at any time. And on the website right now, they do have three different reasoning models to choose from. I'm assuming it's going to jump between these different models when you choose different ones. But if you press the download option right here, there's three different ways you can run this locally on your computer. Again, I recommend just doing the 20B for most computers. If you have a super high-end one, I'll do a dedicated video about the 120B on a different desktop. So, you could run it with an app called Olama, which has gotten a million times easier than the last time I covered running local models with Olama. They have something called LM Studio which is another app you could install and run locally on your computer. They both work for Mac and PC. And then you also have the hugging face that I showed you again more technical. Now I've already installed it with Olama and LM Studio. So let me show you Olama right here. The way you do this is you just go to.com to this website. This is free too. So you're just going to download this to run open source openweight models. Just go ahead and download this for Mac, Windows, and Linux. As soon as you download and install it, this is what the app looks like. The Olama app and you don't have to open terminal. You don't have to literally do anything. You'll see this dropdown right here. And the GPT, the open weight models are going to show up right on top. They're actually not installed on your computer yet, though. And you'll see other open source models, too. You could do DeepS and all these other ones. Llama is here, too. So, all you have to do is have this selected. And if you just type in a message, it will ask you, I'll show you a screenshot what happened when I did it. It will just automatically start downloading it. Okay? And as soon as it's downloading, it's ready to use. That's it. You now have the new chat GPT open model running on your computer locally using Olama. Now, they also have a web search function right here. Right now it's not working very well, but I think that's just because it just came out. But you could turn it on here and you just have to make a free Olama account in order to use this function here. And then you just type in any text prompt and any previous text prompts will appear over here. And it's going to be very limited as far as the functionalities you get inside of chat GPT like file upload and all that type of stuff right now. But you do have this web search function that's only available in this Olama version of it. If I go to the GPT version of it, you see I don't have any type of search and it looks like that first answer is still not working. So, I highly recommend you run this on your computer instead of this website right now. Okay. Now, let me show you the speed of it. Let's go ahead and just paste the simple prompt here. And I'm going to turn off web search. No Turbo. Turbo I haven't actually tried, but we'll try in a second. Again, the model that I just downloaded. I'm going to send this out. It shows you the thinking and you see it was almost instant here on my laptop. And there you go. We got the answer. Three Rs in Strawberry. I actually tried to misspell this to see if it gives me four. But I I will do a deep analysis and compare to things like DeepSeek in a bit. Right now, I just wanted to show you how to install it and get it running. So, this is one way to do it. Let me actually try with the turbo mode turned on. Let me see if it's any faster. Oh, looks like I have to upgrade to a paid version of Olama for that. So, I won't test that right now. But this is just Ola has nothing to do with the model that we have. And then LM Studio. LM Studio kind of is the same way. Olama, I think, is a lot easier, but you can download this for your computer in a similar way, but if you use the LM Studio, it's a little bit more complicated. It will actually ask you to download something called LM Studio CI. So, it says you have to run LM Studio at least once before you could use LMS. And this will require you to open terminal. So, all you have to do is look for the terminal app. And you'll just take this code right here for Mac and this one for PC. If you're using PowerShell instead of terminal, you type that in over here in terminal. It's going to show you this right here. And then all you have to do is on the previous page, you just take this code, this one right here, go back over here, paste that, and it will download everything which I've already downloaded. So this model will be downloaded. So now if I open LM Studio, and if I go to this discover link right here, you'll see that model appears right on top over here. And then I could use it in chat. Now I'm inside of this model right here. And this works in a very similar way as a you send a text prompt, it's going to give you an answer here. So, it's really up to you which one you like. I prefer Olama. I've used it for a very long time, even when it was far more technical. But that's how you run these models locally in a really simple, non-technical way. And stay tuned because I'm going to make a video showing you the 120B model running on my desktop. And then I'll do a lot of different tests and see how it compares to things like DeepSeek R1. And Chad GPT has had some really hidden updates that I covered in this video,

===== END TRANSCRIPT =====
===== BEGIN TRANSCRIPT =====
Video ID: x3u1d6pI-HM
Language: en
Source: auto-generated
Saved At: 2025-08-15T19:16:30.194461
Introducing GPTOSS. It's a open weights model from open AAI. You can run the model locally like this. It's a mixture of X model with 128,000 context window. It's available in two different version 120 billion and 20 billion. When compared with other 034 model, this 120 billion is in par with the other models. With tools is getting higher accuracy compared to without tools, which means this has more agentic behavior. For other benchmarks such as AIM 202425, the accuracy is relatively similar. For PH level science questions without tools, this one beats 03 mini. In function calling, it beats O4 mini. In this, we'll see a quick overview of the model, how you can install this locally on your computer, and how you can integrate this with your own applic. That's exactly what we're going to see today. Let's get started. In regards to competition math, longer the chain of thought, the accuracy is higher. Even for PhD level science questions, longer the chain of thoughts, higher the accuracy. It has built-in thinking mode. They have their own prompt format which you can see here. It's open sourced. It's available in GitHub. And here is how it looks. So generally a large language model will generate text like this. And these information can be extracted or passed such as the start and end of the response, start and end of the message, start and end of thinking. Here is another comparison in artificial analysis.ai. You can see the intelligence is in the top 10. Considering this is a open weights model. Similarly, for speed, it's next to Gemini 2.5 flash reasoning and this has the lowest cost. Here is a comparison GPD OSS and this doesn't support image input which means it's not multimodal. This model is now provided by many providers such as Grock, Fireworks, Together and others. And the cost is very cheap. The 20 billion is much more cheaper. 05 cents for million input token and.20 cent for million output token. So that is the cheapest and we also going to try Gro towards the end. Next we are going to see how you can install this locally. One of the main tool which I regularly use to run any model locally is O Lama which you can download from Olama.ai. So after downloading install that and the application looks like this. So as soon as you type your question it'll automatically download the required model that is I chose 20B. You also have 120B but my current spec is M2 max 32GB and 1TB storage. So in that I can see it's comfortably running. But when I try to integrate this with other application sometime my computer doesn't work fast. So you need much more powerful computer than this to get this run smoothly. There's another option to download. Go to your terminal there. You just type run gpts and that will automatically download the model and you can just try that. As simple as that. Next I want to show you lmstudio.ai. So here you can download this application and GPOSS is available. So click on download and install it. Here is the LM studio application. Here you can search for more models in the discover tab. There I can see GPT OSS 20 billion. Here I can click and then I can download at the bottom which I've already done that. So I can directly chat from here. choosing the model and then you can ask question and it thought for a few seconds and then here is the rest that is really nice finally I want to show you how you can integrate this with other application such as Python applic essay about AI in thousand words so here's the model I'm mentioning GPOSS 120B the reasoning effort medium and only need this much amount of and you have successfully integrating this model with your Python application and I'm going to run this export your gro API key like this you can generate the API key from grock.com then python app.py Pi and then click enter. Now you can see the response is getting generated and it's super fast and I really like this. To create the user interface I used grad imported gradio as grided the code inside this generate function and I am adding this generate function to this parameter and adding three inputs. One is prompt input then temperature and then max token slider and finally a output box. So three different things and demo.launch to launch the applic. I'll put all this code in the description below. So now I'm going to run this in your terminal pip install gradio to install the package. Then python.py and click enter. And here is the URL. Going to open this. And here is the user interface. I'm going to click generate. And you can see the speed in which the essay is getting generated. That is really nice. You can also fine-tune this model. And we've got the detailed code here. And it's using hugging face. And if you want me to go in detail regarding this tutorial, do let me know in the comments below and I will create a dedicated video on how to fine-tune this model. And the key three things which I like about this is its intelligence, speed and also the price. Do try and let me know in the comments below what you think about this. Considering you already like GPT OSS. I also created another video about agentic rag which I highly recommend for you to watch. I'll put the link in here and I will see you

===== END TRANSCRIPT =====
===== BEGIN TRANSCRIPT =====
Video ID: YnVSGhfUfDc
Language: en
Source: auto-generated
Saved At: 2025-08-15T19:16:30.851203
Hello and welcome to the AI with Arun Show. In this episode, we are here to talk about a significant development in the world of AI. On August 5th, 2025, OpenAI made a pivotal announcement, releasing its first openweight models since 2019. For those unfamiliar with the term, an openw weight model means that open AI is making the core components, the brains of the AI publicly available. This allows developers, researchers, and even hobbyists to build upon, modify, and run these models themselves. Now, this is a major step towards making a powerful AI more accessible to everyone. The two new models are called GPS 120B and GPS 20B and they were released under the Apache 2.0 license. So let's meet the two new models that are making waves. First we have GPS 120B. The 120B refers to its approximately 117 billion parameters. Now think of parameters as the individual pieces of knowledge and skills the model has learned. In simple terms, more parameters means a very or a more powerful and knowledgeable model. This larger model is a powerhouse achieving performance that is nearly on par with OpenAI's O4 mini model. Next, in its smaller, more nimble sibling, the GPT OSS 20B, which has around 21 billion parameters, its performance is comparable to the O3 mini model. Now, both of these models share some impressive underlying technology. They have a massive 128,000 token context window, which means they can read and process a huge amount of information at once, roughly the size of a 300page book. They also use what's called a mixture of experts or MOE architecture and a special type of compression called MX FP4 quantization to stay efficient. We'll touch on what that means a little later. Now, let's talk about what this models can actually do, particularly the larger 120B model. The performance is truly impressive. On a benchmark called AIME 2024, which basically tests advanced mathematical problem solving, it scores an incredible 96.6%. For you coders out there, it achieved a code forces ELO rating of 2622, which is a very high competitive programming rank, showcasing its exceptional coding and problem solving abilities. And on GPQA diamond, which is a benchmark designed to be extremely difficult for even the most advanced AI, it scored 80.1% proving it can handle graduate level questions with high accuracy. So, what makes this release so exciting is not just the power, but the accessibility. The smaller model GPT OSS 20B has remarkably low hardware needs. It can run on systems with a minimum of just 16 GB of RAM. This means it can even run on modern MacBooks. In fact, on an M4 Pro chip, it can generate text at a speedy 33 tokens per second. The larger 120B model naturally needs more power, requiring a GPU with 80GB of VRAM for optimal performance on a single card like an Nvidia H00. However, it also has multiGPU support and AMD has already announced day zero support for its consumer processors which is a fantastic news for the broadening access. Of course, as we have said earlier, with great power comes great responsibility. OpenAI has put a strong emphasis on safety with this release. They have applied their preparedness framework which involves extensive evaluation and adversarial testing to understand and mitigate potential risks before release. Furthermore, the models include built-in safety measures. This includes filtering out sensitive data related to chemical, biological, radiological, and nuclear threats and programming the models to refuse to generate harmful content. So why is open AI doing this? Now there are a few strategic drivers. First, there is the element of competitive pressure. This move is seen as a direct response to other powerful open-source models emerging globally such as DeepSync R1 from China. Second, it's about market positioning. By providing these models, OpenAI is helping to establish US-led rails for the global AI infrastructure, setting a standard for how these powerful systems are built and deployed. And finally, and perhaps most importantly, this is about fostering a vibrant developer ecosystem. By offering affordable and highly customizable AI solutions, OpenAI is empowering developers everywhere to innovate. Let's ground this in what it means for real world use cases. We're going to see a leap in a gentic workflows. This means more capable AI assistants that can perform complex tasks like calling functions, browsing the web for live information, and even executing code to solve problems. For businesses, this is a gamecher for enterprise applications. Companies can now deploy these models locally on their own hardware, fine-tune them with their own private data, and do it all in a very cost-effective way. And for academic and research communities, these models will accelerate research and development by enabling rapid prototyping and new avenues of exploration. One of the most disruptive aspects of this release is the economic advantage it offers. The cost is incredibly competitive. To put it in perspective, running these models can be as low as 100th the cost of using a comparable closed source model like cloud 3. Let's look at the numbers. The powerful GPIX 120B cost just 0.15 or 15 cents per million tokens and 75 cents per million token output. A token is a piece of a word. So a million tokens is a massive amount of text. The smaller, more efficient GBT OSS 20B is even cheaper at just 10 cents per input and 50 cents per output per million tokens. This release also brings some key technical innovations to the forefront. We mentioned quantization earlier. The models use a native MX FP4 quantization format. In simple terms, this is a highly efficient compression technique. It significantly reduces the amount of memory the models need which is what helps them run on more accessible hardware all while maintaining high performance. OpenAI also introduced the harmony response format. This is a standardized way for the model to deliver its answers and it comes with open-source tools to render the output. This will make it much easier for developers to get different AI tools and applications to work together seamlessly. Looking ahead, the implications of this release are profound. First, it validates and adds tremendous momentum to the open-source ecosystem. It shows a commitment from a major player that the future of AI development is collaborative. This leads to the two to the true democratization of AI. By lowering the barriers to entry, it allows smaller organizations, startups, and university labs to work with state-of-the-art technology that was previously out of reach. And finally, this will undoubtedly spur infrastructure competition. As more people and companies start running their own powerful AI models, it will intensify competition among the cloud providers and hardware manufacturers to offer the best, most efficient, and most affordable solutions. Thank you for your time. This is an exciting moment for AI and the release of GPD OSS promises to unlock a new wave of innovation and accessibility for everyone. Like, share, subscribe and support our work by joining us as a member.

===== END TRANSCRIPT =====
===== BEGIN TRANSCRIPT =====
Video ID: rSrzv7R2-MA
Language: en
Source: auto-generated
Saved At: 2025-08-15T19:16:31.569041
Hi, welcome to another video. So, OpenAI has finally launched their own open weights model. This is one of their very few open models and probably a good frontier model with open weights. It has some quirks as well and I will talk about them as we go forward. First of all, let's talk about the config of these models. There's the 117B model and then there's the 21B model. These models are designed for powerful reasoning, agentic tasks and versatile developer use cases. Both of them are mixture of experts models and use a 4bit quantization scheme. The 117b model only gets 5.1 billion active parameters while the 20B model gets 3.6 6 billion active parameters respectively. The 120B fits in a single 80GB GPU and the 20B fits in a single 16GB GPU. This is awesome because MacBooks can easily run the bigger variants while consumer GPUs can probably run the 20B model easily. These models are reasoning texton models with chain of thought and adjustable reasoning effort levels. You can easily set the reasoning effort between low, medium, and high based on your specific use case and latency needs. They are good at instruction following and have pretty good tool use support. You also gain complete access to the model's reasoning process, facilitating easier debugging and increased trust in outputs. It's not intended to be shown to end users. Also, these models come with an Apache license which is awesome as it's highly permissive. It also has tool calling capabilities as well and performs well in that regard. You can use this model with everything including Olama, LM Studio, or VLLM, so you can get started with it right away if you need to use it. They say that the GPOS 120B model achieves near par with OpenAI 04 Mini on core reasoning benchmarks while running efficiently on a single 80GB GPU. The GPTOSS20B model delivers similar results to OpenAI 03 Mini on common benchmarks and can run on edge devices with just 16GB of memory making it ideal for ondevice use cases, local inference or rapid iteration without costly infrastructure. These models are not multimodal. So keep that in mind. If we take a look at the benchmarks, then OSS always scores near or below the O4 mini, which is fine considering that this is a small-sized model and open weights as well. You can actually use this model for free as well on the GPToss site that they have made in order to use it and try it out without downloading or running it locally. You'd have to log in here with your hugging face account. So, just do that. And then you can select between high reasoning, low reasoning and mid reasoning as well and then just talk with it as well. So that is kind of awesome. If you do want to use it locally, then the best way would be to use Alma as you can just run this one command and get it cloned locally and run it. Most probably you should be able to run the smaller model locally quite easily which is awesome for sure. As I'm making this video multiple providers have started to give access to this model that is Gro Cerebras and Fireworks. Gro is charging about 15 and.75 for the model which is insanely cheap. Meanwhile, Cerebras is charging about 25 and09 as well, which is also pretty cheap. So, these are the ones available. And you can easily use it via Open Router as well right now, which is quite awesome. Though, you can't yet set the reasoning effort in these models, which is a bummer. Anyway, I was able to run it on my benchmarks, which have been revamped with about 10 new questions. And the only model to score the highest yet is Gemini 2.5 Pro, which scores about 50%. And this model scores pretty low. Now, keep in mind that this is without reasoning because I couldn't find any provider who allows for that yet. So, let's go through these questions. So these two math questions are from Amy and no model can answer this yet and this one also fails which is a pretty big bummer but that's fine. Similarly I also have a riddle here which goes something like I leave men's lips but am no word. I season food but am no herb. Ascend the skies but am no star. I make men weep but leave no scar. And it is pretty simple. The answer to this is smoke. And it solved this which is the only question that it solves. But then most of the benchmark is now coding and that too mostly 3JS. Here I ask it to make me a floor plan using 3JS which is quite a challenging thing to do even for sonnet. And actually no one apart from Gemini passes this. Anyway, here it failed because it renders nothing. Similarly, I asked it for an SVG of a panda with a burger and it doesn't make that any well. A pokeball in 3JS also doesn't render. A chessboard with an autoplay feature is also a pretty bland fail as even the board doesn't render. Web version of Minecraft is also something that doesn't even render anything. while a flying butterfly in a garden in 3JS is also not rendering. And then a CLI tool in Rust for image conversion is also a fail. It only passes one question in my bench, which unfortunately makes it score the lowest amount in my benchmarks. Now this is the non-reasoning variant and it would be only fair to compare it with another non-reasoning model with about the same parameters and that is GLM 4.5 air. So GLM 4.5 air is a model that is actually good in a lot of ways compared to GPOSS even though it scores a tiny bit above GPTOSS in my benchmarks. For example, the floor plan thing. You can see that I gave it a fail, but if you see the generation, then it is very close to perfect. I only give a pass when the plan is usable. But this is so close to being usable. It renders, works, and does everything that you'd want and can be fixed in two or three more prompts, but GPOSS never renders. I'd say that the open AI model is a bit better at SVG generation, but if we dial back and look at the Pokeball, then again, GLM 4.5 pretty much nails it. Whereas the Open AI variant is not even rendering. Then the chessboard, it is again rendering pretty well, and the autoplay here also works, which is something that even the O4 Mini with reasoning fails to do at times. And the OSS model obviously fails. If we have a look at the Minecraft, then it doesn't render, but it does indeed flash some stuff and it just feels better code-wise as well. If we look at the butterfly test, then yes, it works well. You can see that the butterfly here actually flies over flowers and it actually works well without any issues. Similarly, the CLI tool for image conversion also works in the CLI which is awesome. It isn't good at maths or stuff right now, which is fine, but that is what it is. So, even without reasoning, evaluating both tells you that though OpenAI's model is good, it is not anywhere near what open models that we already have. GLM is a prime example of this. You can run the air model on the same hardware as OpenAI's counterpart while getting better performance. I have liked the air model a lot and I was looking forward to OpenAI's implementation but it isn't as good. This is the same Horizon alpha model by the way. So yeah, it should be good at front end but 3JS and backend coding is basically out of its league. I'll see if I can evaluate both GLM 4.5 air and GPT OSS with reasoning. And I have used GLM 4.5 air with reasoning and it gets way better. And I'll check that with GPToss as well. But the first impression is surely not good. I am yet to use the smaller model and that might be good. But we'll see about that because it needs to go against Quen 3 coder flash. I thought to share my thoughts with you guys as well and let me know if you guys like the new benchmarks as well. Overall, it's pretty cool. Anyway, share your thoughts below and subscribe to the channel. You can also donate via Superthanks option or join the channel as well and get some perks. I'll see you in the next video. Bye.

===== END TRANSCRIPT =====
