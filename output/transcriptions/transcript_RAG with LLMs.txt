===== BEGIN TRANSCRIPT =====
Video ID: KurGnK76-EU
Language: en
Source: auto-generated
Saved At: 2025-08-17T18:24:52.433848
Let me explain what is agentic rag using a very simple real life example we will also see the difference between rag and agentic rag. We all know that when we have llm like GPT gemini etc it is trained on a general internet knowledge. So if you ask this question plan a three day trip to Australia it will be able to answer it. But let's say you are creating a chatboard for a car insurance company
called Gerto and if you take a general GPT and if you ask a question okay I have car insurance what is the process to file a claim it will not know that let's say this is a new company and GPT or claude has not scrapped the data from their website it's a new company so it doesn't know the process of how to file a claim so let's say if you're working as an AI engineer in this Gerti Deco company and
if you're building a chat board which can assist your customer one thing you can do is you can provide access of your policy documents to this LLM this is policy knowledge base okay and when you provide access of this knowledge to LLM it will be able to retrieve the answer now as an AI engineer you will build a rag application retrieval augmented generation application where for this question you
will retrieve the relevant chunk. See this database might be very big. Okay, let's say 1 TBTE 1 GB let's say. Now in LLM we have a context window limitation. So you can't feed all the PDF. So you will do semantic search. Okay, you will do semantic search and for this people use vector databases. So what they do is all these PDF they will index into a vector database. Let's say this is your vector
database and in vector database you will do your semantic search and whatever relevant chunks or relevant policy document information that you have you provide it in a context. So when you ask a question to LLM uh what you do is you provide your question here okay and you will say provide me the answer based on this context. So in this context whatever relevant chunks you have retrieved you will
provide it and then it will be able to uh provide you the answer. This is traditional rag. Now let's say you ask this question why did my invoice go up this month? This is a different question. And as an AI engineer now you want to build something uh intelligent something which is agentic. Okay, see previously all these things you have coded it up in your Python code. Let's say maybe you use lang
chain. Okay, and Python and you have done coding of all of this. So it will do just one retrieval. Whatever chunks come you will have to find the answer. But now you are building an agent. So in agent you will provide a number of tools. So these tools can be see you can have your knowledge base you can have your billing database you can provide access to your API for usage data and this entire
system that you're here that you're seeing here right this entire thing is actually an agent and you leave it up to this LLM to make a decision when to call policy knowledge base when to call billing database etc. So when you ask this question now LLM has a brain it's intelligent it will first decide okay now first I need to get some policy document data I need to also get billing information for
this customer so it will identify okay this this customer ID is a 3459 for which it will make a call you are not doing this in Python by the way LLM is making a call you are just setting up an agent using langraph agno whatever is the framework network but LLM is making autonomous decision to call to a specific knowledge source for a given need and for the usage it might call API. Okay, not only
that it might call these things multiple times. So see here also a retrieval augmented generation is happening. It is retrieving records from this PDF from this database etc. But compared to rag here the selection of database LLM is making autonomously. So there is autonomous behavior. There is agentic behavior here. Second difference is in a rag you just make one call one call retrieve chunk and
that's it. Here you might make multiple calls. So first it will make API call it will retrieve the data. Then it will call building database get some data. Now let's say if it doesn't find satisfactory answer, it might call API again. It might call billing database again. It's a multiple iterative approach until the goal is satisfied. So eventually it will have an answer that your users exceeded
this. So your invoice increase this. Now this is simple Q&A. It can also perform an action. Customer might say upgrade my plan to this. It will actually upgrade. So agentic rag is little smarter compared to traditional rag. So to summarize in rag you are retrieving context. Agentic rag also you are retrieving context. Okay but in rag you don't plan next steps. You don't do multi-turn reasoning.
You don't uh you rarely use tools and APIs. Okay. You have knowledge source and there is no task autonomy. Here in agentic you can actually complete the task. You can upgrade the plan also you know and you can use multi-turn approach. So folks that's what it is. Agentic rag is basically agentic in nature. It can have multi-turn approach. LLM has autonomy to select which database to call etc.
Whereas rag is more rigid. It is all done through coding and it's not agentic in nature. I hope this video gave you some clarity on the difference between the two. If you have any question, post in the comment box below. If you like this video, give it a thumbs up. Share it with your friends.

===== END TRANSCRIPT =====

===== BEGIN TRANSCRIPT =====
Video ID: tSYMGsQh7QI
Language: n/a
Source: description-fallback
Saved At: 2025-08-17T18:24:53.177495
·¥ò ü·¥á·¥ÄÍú±·¥á  üÖªüÖ∏üÖ∫üÖ¥, üÜÇüÖ∑üÖ∞üÜÅüÖ¥  & üÜÇüÜÑüÖ±üÜÇüÖ≤üÜÅüÖ∏üÖ±üÖ¥ ·¥õ·¥è  üÜÉüÖ¥üÖ≤üÖ∑üÖ≤üÖøüÜÇ ·¥Ä…¥·¥Ö ·¥Ö·¥è…¥'·¥õ Íú∞·¥è Ä…¢·¥á·¥õ ·¥õ·¥è ·¥ò Ä·¥áÍú±Íú± ·¥õ ú·¥á  ô·¥á ü ü …™·¥Ñ·¥è…¥Íú±. ·¥õ ú·¥Ä…¥·¥ãÍú±. ‡§ú‡§Ø ‡§π‡§ø‡§®‡•ç‡§¶, ‡§µ‡§®‡•ç‡§¶‡•á ‡§Æ‡§æ‡§§‡§∞‡§Æ üáÆüá≥|  üéØ Welcome to Techcps - Google Cloud Qwiklabs Solution Tutorials!   In this video, we walk you through the complete solution for the lab:   "Build an LLM and RAG-based Chat Application using AlloyDB and LangChain" ‚Äì Qwiklabs lab GSP1226 Perfect for learners in the Google Cloud
Ready Facilitator Program and Arcade challenges!  ---  üìé Important Links: üîπ Lab Link ‚Üí https://www.cloudskillsboost.google/focuses/93570?parent=catalog  üîπ WhatsApp Channel‚Üí https://whatsapp.com/channel/0029Va9nne147XeIFkXYv71A    ---  üéÆ Explore More Playlists:   üî∏ Base Camp ‚Üí https://www.youtube.com/playlist?list=PLwk5HW8luxkz4HmROQday8IqdtPis_HwN   üî∏ Level 1 ‚Üí
https://www.youtube.com/playlist?list=PLwk5HW8luxkzJXqn3QaOdkabGdEeZhW3n   üî∏ Level 2 ‚Üí https://www.youtube.com/playlist?list=PLwk5HW8luxkwGowQezvA3n6VsC_tOJhsK   üî∏ Level 3 ‚Üí https://www.youtube.com/playlist?list=PLwk5HW8luxkxluqah5VHYutNXYYEW7teE    üéØ Arcade Trivia Weeks:  Week 1 ‚Üí https://www.youtube.com/playlist?list=PLwk5HW8luxkw4-4VKKL4-OdCUDTpi8WKG   Week 2 ‚Üí
https://www.youtube.com/playlist?list=PLwk5HW8luxkxRTKWWfXoILL3jDAiKLOIP   Week 3 ‚Üí https://www.youtube.com/playlist?list=PLwk5HW8luxkxgsRaubPew8ZeXxKGsKxDY   Week 4 ‚Üí https://www.youtube.com/playlist?list=PLwk5HW8luxkzkPGd6yW5bm_PcgcoHBCmp    üéì Certification Zone ‚Üí https://www.youtube.com/playlist?list=PLwk5HW8luxkwvGHuHCvhmf2OaCvXNFn8l    üéÆ 2 Points Game ‚Üí
https://www.youtube.com/playlist?list=PLwk5HW8luxky_2hRJGjieCBAf8kXhOHo-  üìå Resources & Docs: üìÑ Arcade Games Doc ‚Üí https://docs.google.com/spreadsheets/d/1lDMwNcZCEqkMC_BvS_WFDgzb77AZENtXO9g9sy2Uws4/edit?gid=2139948920#gid=2139948920   üìÑ2025 Skill Badges list ‚Üí https://docs.google.com/spreadsheets/d/1lDMwNcZCEqkMC_BvS_WFDgzb77AZENtXO9g9sy2Uws4/edit?usp=drivesdk   ---  üì≤ Join Us on Social Media:
üî∏ Telegram Channel: https://t.me/Techcps   üî∏ Telegram Chat: https://t.me/techcpschat   üî∏ LinkedIn: https://www.linkedin.com/company/techcps/   üî∏ GitHub: https://github.com/Techcps   üî∏ Twitter (X): https://twitter.com/Techcps_   üî∏ Instagram: https://instagram.com/techcps   üî∏ Facebook: https://facebook.com/techcps  ---  ‚ö†Ô∏è Note:   This video tutorial was created using materials from **Google Cloud
Skill Boost**. All rights and content credit goes to Google Cloud. This video is for educational purposes only. If you're the rightful owner of any content and wish for credit or removal, please contact us directly.  üìú Disclaimer: Copyright Disclaimer Under Section 107 of the Copyright Act 1976, allowance is made for "fair use" for purposes such as criticism, comment, news reporting, teaching,
scholarship, and research. Fair use is a use permitted by copyright statute that might otherwise be infringing. Non-profit, educational, or personal use tips the balance in favor of fair use.  ---  Google Cloud Skills Boost, GSP1226, Cloud Arcade Challenge, Google Cloud Facilitators, Build with GenAI, LangChain tutorial, AlloyDB tutorial, LLM RAG app, GenAI on Google Cloud, Chat App with
LangChain, RAG on AlloyDB, Google Cloud Labs, Facilitators Challenge 2025, Google Cloud Tutorial, LangChain and AlloyDB, Build AI Chatbot, Generative AI Google Cloud, Google Cloud Training, LLM with RAG, GCP Generative AI  --  üîñ Tags: #techcps #GoogleCloudFacilitators #CloudArcade #GoogleCloudSkillsBoost #GSP1226 #GenAIonGoogleCloud #AlloyDB #LangChain #RAG #LLMApps #BuildWithAI #GenAILabs
#CloudSolutions #FacilitatorsChallenge

===== END TRANSCRIPT =====

===== BEGIN TRANSCRIPT =====
Video ID: VioF7v8Mikg
Language: en
Source: auto-generated
Saved At: 2025-08-17T18:24:54.099685
Have you ever tried asking who won the IPL in 2025 or explain the code I wrote last week and what happens? Nine times out of 10 it just starts hallucinated just making stuff up going completely off the rails. Well, if you used any LLM in the past years, whether it's chat GPT, Claude, Gemini, Grock, Mistral, whatever, you've probably run into this one big annoying problem. You ask something super
specific like a detailed question, something about yourself, some code you wrote last week or a spreadsheet that you uploaded and the model answers super confidently, like it knows everything, but it completely misses the point. Sometimes it just straight up hallucinates and gives answers that don't even exist. And look, the reason is dead simple. Large language models are pattern matching
machines. They're incredible at regurgitating what they've already been trained on. But here's the kicker. They don't know your data, your context, or your secret source. And this is exactly why AI is still struggling to make a massive dent in fields like law, medicine, and compliance. You know, the places where hallucination isn't just an oops, my bad kind of situation. It's downright dangerous.
Because let's be real, when you yank out your context, that fancy AI model just becomes generic. It becomes mid. But there's got to be a fix for this, right? We can't be pushing AI this hard and just leave this massive huge problem hanging. So, what are we going to do? We've actually got two solid ways to tackle this. And today, we're going to break them down for you. Ways of fixing AI. All right,
so we've got this context problem. How do we actually solve it? It turns out that we've got two main players in the book. So, let's dissect them. The first option that we have is fine-tuning. Think of this as sending your AI model back to school. But this time, the curriculum is all about you. You literally take that base model and retrain it from the ground up with your own data. Like your
emails, your entire code base, your chats, your pictures, everything gets thrown into the mix and it literally learns your specific domain and becomes a native. The upside is massive. Once the model is trained up, it's like it was literally born for your use cases. You don't need to keep spoon feeding it and giving it extra context every single time. It just gets it. but and it's a big butt. It
can be extremely painful. Seriously. So, GPU time is going to cost you an arm and a leg. And what happens when your data changes? New data, new code, you guessed it, back to square one. Repeat the entire process. And plus, managing versions of these huge model checkpoints is a messy logistical nightmare. Trust me. So, that brings us to option number two, which is RAG. And RAG stands for retrieval
augmented generation. And folks, this is where things get really, really interesting. This is the street smart agile cousin. Way way simpler. You don't even need to touch the underlying base model. No expensive retraining. Instead, you just build the clever context engine. And you can just think of this as a superefficient research assistant that sits around the LLM. And then at runtime, when a
query comes in, the engine zips in and feeds the model just the right pieces of information it needs right when it needs them. Let's imagine that you're a world-class chef. You know how to cook anything, but you don't know what the next order from the dining room is going to be. With Rag, the moment that order hits the kitchen, bam, someone magically hands you the perfect detailed recipe for the
exact dish. You didn't even have to deal on cooking. You just got the precise instructions that you needed. That is Rag right there. That's the power. No retraining, live updates, and way cheaper. So, now you guys understand the beauty of Rag. But why does this setup work so incredibly well? Why is it becoming the go-to for so many people trying to make LLMs actually useful with their own data?
Why does Rag work so well? And here are the reasons. Number one is fast iterations, new docs, no sweat. Add them. Re-mbed them and your rag will instantly get smarter. No waiting for weeks for a retrain. Next is cheap infrastructure. Forget burning cash on endless GPU cycles. Rag is lean, minimal compute and your wallet will always stay happy. Next is it's always fresh. Your info never gets stale.
upload a doc, your rag adapts in seconds and always with the latest intel. So you get speed, you can save cash, and your AI always stays current. That's a pretty powerful combo. Okay, so now you're probably thinking, okay, this sounds cool, but how does this rag magic actually work under the hood? Don't worry, we've got you. Rag pipeline. Okay, so how does this rag wizardry pull off giving your LM
the brains it needs without the pain of retraining? We're going to break down the entire pipeline. And to make sure it's super easy to lock into your memory, we're going to use an analogy. Imagine you're setting up the most insanely organized high- techch library ever built. And for all you visual thinkers out there, we've created this crazy crazy massive diagram. So, we're going to drop a link so
you can explore it on your own later, but for now, let's walk through it together. All right, let's dive in. All right, so step number one is your data intake. Imagine this being the part where the books arrive at the library. The first things first, your data. This is where all your books start showing up at the library doors. Think of your company's PDFs, your email archives, critical CSVs, even
your entire codebase, all your content. So consider this to be your raw materials, the books that need to be cataloged in our super library. Now we move on to step two, which is chunking. Now imagine this is where you're breaking down the books into index cards. Now you're not just going to cram the entire encyclopedia onto one shelf, right? So, you take each book, each document, and you chunk it.
You break it down into smaller bite-sized pieces. And you can think of them as individual index cards, maybe one paragraph per card or logical section. And the key is digestible pieces. Why? So, instead of your AI librarian having to flip through 300 pages to find a single answer, it can search these cards way faster and way more effectively. Precision, people, it's all about precision. So the
tools that you can use for this is lang text split or you can also use llama index. Now we're moving on to step three which is embedding. Now imagine this to be the part where you're giving each card GPS coordinates. Now this is where the real AI magic starts to kick in. We take those text chunks, those index cards and we run them into coordinates. Now think of it as assigning a super precise GPS
location to every single piece of information in your library but for language. The trick is that the cards with similar meaning get plotted in nearby locations in this massive multi-dimensional space. So words like similar, same, identical, they're all hanging out in the same neighborhood. Popular models that you can use for this is Google's text embedding API or you can also use OpenAI's text
embedding 3. So you have a lot of horsepower to choose from. Now we're moving on to step number four, which is vector storage. Now this you can imagine as organizing the high-tech shelves. All right, so our Index cars now have their GPS coordinates. So, next up, we're going to need some serious shelving to store them. And this isn't your grandma's dusty bookshelf. This is a high performance vector
database. So, you've got names like Pine Corn, Chroma, Qentrint in the Ring. Pick the one whose landing page you vibe with the most or the one that fits your scale and budget. Seriously, they're all pretty good. And it doesn't matter if you got a,000 cards or 10 million. These databases are built for speed. They can use semantic searches, finding those relevant meaning coordinates in milliseconds.
Blink and you'll probably miss it. Okay, now we're moving on to step number five which is retrieval. Imagine this to be the part where the librarian finds the exact cards. Okay, so now your library is set up. Now user walks in with a question. So after the user asks that questions, what do you think is going to happen? So first the rag system takes that user's query, embeds it and turns it into a
vector just like it did with all your documents. Then it performs a similarity search against your entire vector database and does something like show me the top five or six cards whose content is semantically closest to this question. So those are going to be your golden index cards with each one of them holding a crucial part of the answer and also a relevant snippet of information. Now we're
moving on to step number six which is synthesis. This is the part where the librarian writes the perfect answer. This is where our super smart LLM, our AI librarian steps up to the plate. We feed it those top rank relevant chunks plus the original user query and we usually give it a little nudge, a guardrail prompt so to speak, something like use only the context provided. If the answer isn't
there, just say so. The LLM then reads these carefully selected cards, understands the question in that specific context, and spits out focused, accurate, and contextual answer. No hallucination, no wild guessing, and no making stuff up. It's answering like it knows your data because in that moment, for that query, thanks to Rag, it actually does. So now, theory is great. Analogies are fun, but at
Builder Central, we're all about building and shipping. So, now that we've walked you through how Ragg actually works, how about we show you what we actually built using the same exact approach. Ragbot, this isn't a full-blown line by line coding tutorial on how we built this specific chatbot. So, we actually dove deep into any which was our main tool for this in a previous video. If you've missed
that video, make sure you check it out. The link is going to be either in the description or somewhere over here, depends on where the editor puts it. So, we showed how you can visually build these kind of powerful workflows with minimal to no code. So here's our flow for the data source. What we did is we used Google Drive and connected it via GCP. Now the reason we did this is because it enables
us to upload the documents in real time effectively turning it into a live database. For embeddings we used OpenAI to generate them which was really really easy and inexpensive. For storage we use Pine Cone as our main vector storage database because they offer a fairly generous free storage tier. For retrieval and synthesis, we use Google's API, which handled the LLM part by synthesizing answers
based on the embedded chunks that were received. So, what does this actually look like in action? Well, with this setup, you can throw pretty much any file at it. PDFs, Word docs, you name it. The bot chews it up, processes it, and then boom, you're chatting with your own data. So, you need to ask it something specific like, "What's the difference between function A and function B in this massive
codebase I just uploaded?" And it should spit back the answer according to the document. It also works for CVs if you're hiring. So those complicated recipes you can never follow, dense legal documents, your chaotic lecture notes, whatever you've got, just, you know, be smart about it. Don't upload some deepest darkest secrets. Okay, that's kind of stupid. Don't do that. So what would be the end
result? You can literally just drag and drop any document into the Google Drive folder and the chatbot it updates either in real time or on a schedule that you set and then it's ready to answer your questions using that fresh updated context which is pretty cool, right? JSON file for NA10 is in the description. So, make sure you use that and create your own ragbot. Basically, in a nutshell, rag is
making your AI actually know your world. All right, ladies and gentlemen, that's our session for today. Until next time, keep building, keep experimenting, and stay tuned to Builder Central for more such content.

===== END TRANSCRIPT =====

===== BEGIN TRANSCRIPT =====
Video ID: xPMQ2cVbUTI
Language: en
Source: auto-generated
Saved At: 2025-08-17T18:24:54.888800
Hello, I am Brian Samboden and today I will be your guide through the world of RAG or retrieval augmented generation which is the implementation pattern that's enabling us to put the power of large language models or LLMs to practical use. Let's delve into the world of Gen AI LLM's information retrieval and conversational user interfaces. To understand why we need architectures like Rag, we should
start at the beginning and learn about the forces that got us here. Starting with the systems at the core of this new computing revolution, language models. Language models have been around in one form or another since the early days of computational linguistics and artificial intelligence. We can think of them as systems designed to parse, understand, generate or manipulate human language. A way
to make machines and humans interact in the language of humans. Since the beginning, one of the main goals of a language model is to power conversational chatlike user interfaces where the system is prompted with some natural language text input and the language model generates some natural language text output. One of the first notable examples of such a system was ELISA developed in the mid
1960s which could simulate conversations using pattern matching and substitution techniques. Systems like TLISA worked with hard-coded rules. The language model underneath was nothing more than a careful crafted program that could parse the incoming sentences and construct responses from templates and parts of the input. Fast forward to the 1990s when we saw the rise of statistical methods for
machine translation that focused on learning from data rather than relying only on linguistic rules. And by the mid 2000s, several methods, including statistical models and early neural network approaches, were being used to power language models. But it wasn't until the late 2000s and early 2010s, with the advent of deep learning techniques and computational power improvements that neural
networks truly began to take center stage in language processing. Advances in computer vision in the mid210s brought deep learning to the forefront with convolutional neural networks or CNN's. And it wasn't long before language models caught up to the deep learning renaissance. Sequencetose sequence models and the emergence of attention mechanisms evolved to improve the ability to understand
context in language. 2017 marks a milestone in natural language processing with Google's attention it's all you need a paper introducing the transformer model. Soon after in 2018, the first GPT model generative pre-trained transformer was released by OpenAI. GPT was trained on the web text data set consisting of text extracted from millions of web pages likely amounting to several tens of
gigabytes of text data. This training data set included pages from various websites which provided a broad spectrum of language use styles and topics and aimed to capture a wide variety of human language. GPT had a capacity of 117 million parameters. Around this time, the term large language model was popularized to denote the model's capacity, architecture, and training data set size. In the
following years, LLMs have observed an exponential growth trend. GPT2 released in 2019 had 1.5 billion parameters. GPT3 one year later had 175 billion parameters. And in 2023, GPT4 is estimated to have 1 trillion parameters. An LLM is trained to calculate the probability of a word following a given sequence of text. All words are given a weight from the known vocabulary and the word with the
highest weight it's taken as the best candidate to be the next word to appear in the sequence. LLM capture world knowledge within their parameters demonstrating an advanced ability to mimic language understanding. This capability allows them to be adapted or fine-tuned to achieve state-of-the-art performance on a wide range of language tasks. LLMs are central to the emerging field of generative
AI. So what is RAG? RAG stands for retrieval augmented generation for knowledge intensive NLP tasks originated with the seminal paper published in 2020 by Louisal under the sponsorship of Facebook AI research. Let's sparse rag into its constituting components. Starting at the end of the acronym, the part that we have already touched upon, generation. The generation entails assembling a prompt
containing the user's question or query, submitting the prompt to the LLM, and receiving the generated answer. So, why is this mode of operation not enough when building knowledge intensive applications? If LLMs capture the world's knowledge in their parameters, shouldn't they always be able to generate a precise answer to a query? LLM responses sometimes contain information grounded on facts
mixed with speculative and sometimes create minations. These types of responses have been labeled hallucinations. Instances when the model generates somewhat plausible but incorrect information. LLM hallucinations arise from several known and theorized causes reflecting the complex interplay between model architecture, training data, and generation techniques. Also, the accuracy of an LLM's
generated responses heavily depends on the quality of the input data it was trained on. If the training data contains inaccuracies, biases, or is otherwise flawed, the model's output can reflect these. the well-known garbage in garbage out principle. An LLM may also overgeneralize from the training data. Similar to how humans sometimes incorrectly apply a learn pattern to a new context. What
appears to be a clear chain of reasoning, it's rooted in nothing more than learned statistical language patterns. The freshness of the knowledge is also a big issue. By the nature of their lengthy and costly static training process, LLM possess knowledge that it's frozen in time. If discoveries, events, or retractions have occur that could affect an answer, the LLM would not know about them. Even
if the generated answer is factually correct, there is no traceability to the source. Finding what set of documents, web pages, or papers the LLM truth is grounded on becomes a task for the user. LLMs are trained on a large variety of data. For a given topic, the LLM might have learned a combination of expert information mixed with naive comments, misinformation, and outright lies. To improve
answers in a specific domain, for example, a company's inner workings. Context must be provided to guide and ground the LLM's response. And finally, the way a question is worded for an LLM. It's incredibly important. LLMs rely on the provided context to generate relevant responses. If the input prompt is ambiguous, lacks specificity, or the model's input size limits essential contextual
information, the model may hallucinate details to fill in the gaps. Crafting an effective prompt has become both science and art to the point that the term prompt engineering it's common place and prompt engineer has at least temporarily become a job title. And this is where the retrieval part of rag comes into the picture. The R in rag, it's part of the larger field of information retrieval,
which is the science of finding relevant information in one or more data sources in response to a user query. In the context of rag and conversational AI interfaces, the queries are natural language queries. the relevant data. It's typically in the form of unstructured data such as PDFs, word documents, web pages on your company's internet, industry journals behind a gated wall, or company
confidential information. This sounds simple enough, right? But it is not. Accurate and effective information retrieval is a very important and complex component of a rag implementation. So, let's dig deeper into the common implementation of the retrieval component. A very popular implementation uses a vector database. A vector database is a system that enables the storage, management, and
querying of unstructured data. The first step to using a vector database in a rack system is to generate vector representations of the data in our knowledge base. Vectorization is the secret sauce that allow us to standardize, store, and operate on unstructured data. Previously, we learned that language models have the power to guess the next word or token based on some input text. Another feature
that LLMs in many machine learning models possess, it's the ability to generate a numeric representation of the input data. In machine learning, we call this numeric representation a vector embedding. These vector embeddings compress and encode the semantic information of the input and place them in a latent space, a multi-dimensional space where a data point's location reflects the meaning of the
content. When these embeddings are stored in a capable vector database, we are provided with the ability to efficiently search for related content given an input example. We call this kind of retrieval dense retrieval. And in the rack case, it's a form of semantic search. Now that we know the potential pitfalls of throwing an LLM into a knowledge inensive application and we had a high-level
overview of how modern dance retrieval systems can be implemented, let's connect the two halves and tackle the A in rag augmentation. The first step in a typical RAG interaction is translating the user's natural language query into a form the system can efficiently process. We must first vectorize the user's query to search for relevant data in a vector database. The vectorzed input query it's
used to find semantically similar vectors. The k nearest vectors identified in the vector database correspond to specific documents or passages most relevant to the user's query. These documents are then retrieved and the relevant content which could be entire documents, paragraphs or sentences. It's extracted for each of the documents. The extracted content and possibly the original query are
used to construct a prompt for the LLM. This prompt is carefully designed to encapsulate both the query and the context provided by the retrieve documents, guiding the LLM to produce a relevant and informed response. The LLM generates a response that may undergo further refinement to ensure its relevance and accuracy or to meet specific criteria set by the application. This could sometimes involve
additional processing steps such as reranking generated answers, post-processing for grammar and coherence, or even human in the loop review. Finally, the generated and refined answer is presented to the user as the response to their query. Let's focus on the setup of the information retrieval portion of a rack system. More specifically, the indexing process. In order to efficiently perform dense
retrieval based on our user queries, we need to populate our data store with information that will provide the accurate up-to-date context to our queries. As we previously learned, this data needs to be vectorized and stored in a vector database. Creating a proper data ingestion pipeline for rag. It's a complex process that so far we have abstracted and simplified. Let's dive deeper into this
aspect of rag. Let's focus on the typical documents found in an enterprise environment. These documents can vary widely from structured reports to informal chat transcripts each of variable length and with a possible combination of topics. Some documents might be well structured, maybe following a template. Others might be completely organic chat transcripts from meetings. Some might be emails.
Think about the dynamics of a corporate meeting. For example, we all have been in a meeting where the topic and focus of the conversation drifts. The transcript of such a meeting might contain the specific answer to a user's query, but it is now buried in a semantic soup of topics representing the whole document. That transcript if vectorized will place that document in the latent space somewhere
between all those topics potentially eliminating it as a match for the user's query. The level of complexity of storing such a document in a vector database depends on how precise and efficient you want the dense retrieval process to be. There are a couple of immediate issues at hand. The first one is a potential size of the content. LLMs have a fixed context window length which puts a limit to
how much contextual information we can pass as part of the prompt. The second and more important issue when it comes to the relevancy of the information retrieved is the concept of context drift. To deal with both of these issues, one of the simplest strategies is to break the document into chunks and vectorize the chunks individually. The component in charge of generating appropriate chunks is
aptly named the chunker. These chunks are more likely to capture a focused view of the document allowing for a finer grain match against user queries. How we select the chunk size is very important. If chunks are too small, certain questions cannot be answered. If the chunks are too large, then the answers will include a level of noise. The vector database, the vector for a chunk gets stored along
some metadata that points back to the document of origin and the location of the chunk in the document. There are several chunking techniques in use including fixed length chunking, semantic chunking, and querybased chunking. Each offering unique benefits in processing and retrieving information. Now, let's focus on the querying process and break down the commonly found components and their
functions in a rag implementation. Rag is an emerging architecture and advancements are being made every day. So, today we'll cover only the most commonly found components in current rag implementations. The querying process starts with the rag system receiving a user's query. Then a component called the rewriter modifies the original query to improve retrieval results, potentially by expanding it
with synonyms or rephrasing it for clarity or even breaking it into multiple subqueries. The retriever component fetches relevant documents or chunks from the dense retrieval system or other sources in order to provide context. The retriever might contain or employ a router to manage multiple data sources. A reranker can be used to further assess the relevance of the retrieve documents, perform
compression, and better align the context with the user query. The consolidator aggregates and synthesizes information from the top documents, possibly dduplicating or summarizing information to prepare for final processing. At this state, the prompt has been grounded with context and instructions to guide the LLM generation process. Finally, the reader prompts the LLM with the engineered prompt
and interprets the LLM's response to create a coherent and contextually relevant answer for the user. In some RAG implementations, especially those dealing with complex or multi-turn interactions, a contextualizer might be necessary to integrate the current query with the context of a long conversation. With the understanding gained, let's run through a simulated Ragnar action in a fictitious
cinema expert rack power chatbot. Let's say that we want to ask our chatbot whether any of the characters the actor Pedro Pascal ever played had an animal nickname, but our user's wording of the query it's less than perfect. The rack query rewriter might rephrase the query to make it less ambiguous, including keywords like movie, film, TV show, and ro to guide the search process. The retriever
component searches our context store using the constructed query and returns a few articles. Imagine the top match is an article about the Mandalorian TV show and the second one it's about the 2019 movie Triple Frontier. The re-ranker then evaluates the relevance of the retrieve articles based on the query specific focus and in our case it will prioritize a second article which directly addresses
the query. The consolidator then takes the top rank articles provided by the re-ranker and creates a focus summary by extracting the key facts and presenting them in a manner that aligns with the query. Finally, the reader component crafts the prompt that will be submitted to the LLM. Upon receiving a response, it will perform any reformatting, sanitizing or enhancements required by the
application so that we can return a proper response to the user. In closing, retrieval augmented generation or rag represents a sophisticated pipeline of processes that elevate the capabilities of large language models by infusing them with up-to-date, curated, and highly relevant contextual information from external sources.

===== END TRANSCRIPT =====

===== BEGIN TRANSCRIPT =====
Video ID: rqyczEvh3D4
Language: en
Source: manual
Saved At: 2025-08-17T18:24:55.752218
When building GNI applications, retrieval augmented generation is often contrasted with fine tuning as two separate techniques for incorporating domain-specific data into LLM output. Retrieval augmented fine tuning is a hybrid approach that combines the best of both worlds and addresses many of the challenges surrounding LLN performance in specialized settings. Originally developed by researchers
at UC Berkeley. RAF uses a unique fine-tuning technique to improve RAG performance in specific domain contexts. Now with traditional rag, we provide context to the model during inference. By using a retriever to search for relevant documents in a vector database that we append to our prompt that we send to our LLM. With fine tuning, we provide context to the model during training time by using a
large label data set to bake specific knowledge into a pre-trained LLM. So how can we combine both of these techniques to create retrieval-augmented fine tuning? Let's use an analogy. Let's say that using an LLM on enterprise-specific tasks is like studying for an exam. Suppose that fine-tuning is like studying for a closed book exam. Since you can't use your notes, you have to memorize all the
materials in advance. And if you study all the wrong stuff, you probably won't do so well since you don't have access to new information. In the same way, with fine-tuning, the model has to rely completely on the knowledge it learned during training in order to answer the user's question. Now, RAG would be like taking an open book exam. That you did not study for. Because you knew you could use
the book on exam day, you chose to skip all the lectures and not read the textbook. So on test day, even though you have all the materials in front of you, there's still no guarantee that you'll actually be able to know where to find all the information. In the same way with RAG, the performance of the model is largely dictated by how well the retriever can pull relevant documents from the
database. Now, with Raft, this is like... Taking an open book exam that you did study for. This is the win-win situation, where you paid attention in all the lectures, read all the materials, and get to use the book on the test. So RAF is similar in that it teaches the model how to use RAG, or how to external documents to generate an answer. It's like the saying that goes, give a man a fish, and
you feed him for a day. But teach a man to fish, and you feed him, for a lifetime. In the same way, RAF- essentially teaches the model how to fish or how to look for and generate an answer versus just giving it fish or giving it an answer. To explain this more, let's dive into the implementation. Since Raft is a training technique, we need training data. Each data point will consist of three
things, a query, a set of documents, and an answer Let's look at an example. Let's say our query is how much parental leave does IBM offer? To generate an answer, we can search through two types of documents, core documents and tangent documents. Core documents contain information that's relevant to the user query. In our example, these could be documents on, say, pay leave or benefit eligibility.
Tension documents, on the other hand, contain information, that's irrelevant or off-topic to the use of your query. These could be document on, retirement accounts or internal code documentation. From here, we create two types of document sets. Set one. Contains both core and tangent documents, and set to contains just tangent documents. The reason why we include both is to simulate a real RAG use
case where the retriever may or may not pull any relevant documents from the database. Finally, to generate our answer, we use chain of thought reasoning. To teach the model how to filter past tangent documents and focus on and process through core ones step by step in order to generate a correct. We can use this framework to create a larger training data set that we can use to train the model
using supervised fine tuning. Now, because this framework is so adaptable, we can a wide variety of different models and fine tuning techniques to actually implement this in practice. And with that, our model is now ready to ace the exam. So there are three aspects of this training process that I want to highlight that are key to making this whole thing work. One, the inclusion of tangent
documents helps to teach the model how to pick out relevant documents from irrelevant ones, thus helping to increase accuracy on domain-specific questions. Secondly, the creation of document sets that don't include any relevant documents at all, AKA set two. Help to teach the model when to rely on its intrinsic knowledge or to say, I don't know, versus forcing an incorrect answer out of irrelevant
rag documents. This helps to minimize hallucinations. Guiding the model using chain of thought reasoning helps to minimize overfitting and increase transparency and traceability by encouraging the model to quote specific documents from which it got the answer from. So as you can see, Raft creates a model that's both highly scalable and highly robust for enterprise tasks. So whether you found this
video because you're studying for that closed book exam or you're just curious about AI, I hope you learned something and enjoyed the video. Thanks for watching.

===== END TRANSCRIPT =====

