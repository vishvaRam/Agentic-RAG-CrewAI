===== BEGIN TRANSCRIPT =====
Video ID: 3udsYrPheOw
Language: en
Source: auto-generated
Saved At: 2025-08-17T11:39:18.681247
270 million parameters engineered for brilliance. In this video we are going to look at Gemma 3 270 million parameters not billion million parameters. This is a very compact model for hyperefficient AI. If you have any use case which needs focused task to do you can fine-tune this model and use that model for that particular task. It's a 270 million parameter model designed from the ground up of a
task specific fine-tuning with strong instruction following and text structuring capabilities already trained in. You can look at the benchmark the eval benchmark. This benchmark tests a model ability to follow verifiable instructions. We can see Jama 370 million on the top uh comparing it with different model sizes. So on the x-axis we have the model size in parameters in billions of parameters.
We can see that this quen 2.5 0.5 billion instruct percentage on the if benchmark is lesser than jama 3 to70 million. Now these are the capabilities of gemma 3 to 70 million. It has a total of 270 million parameters 170 million embedding parameters due to a large vocabulary size 100 million for the transformer blocks. It has a large vocabulary of 256k tokens. That's amazing. It is extremely
extremely energy efficient. It can work on Pixel 9 Pro So SOC. It shows that it just uses 75%age of the battery for 25 conversations. It's using the 4-bit quantization model for more efficient and low memory consumption. Instruction following. This is actually two models are released. First is the base model and there's the instruction tune model. Well, we can use that instruction tune model to go
ahead and fine-tune. So this is the instruction tune model. Gemma 3 270 million it. So it is instruction tune model. If you want to fine-tune this model on different instruction following data sets, you need to use this model. In this video, we will see a fine-tuning examples as well. Uh I will fine-tune this model and show you how you can fine-tune it easily. I will show you the codes and you can
get started on your own data set. Now, this is productionary quantization. It's using QAT. It's not just like putting quantization at the end of the process. It is a quantization aware trained which means during the training process we are using quantization. We can see that we are using QA and can be used to maintain the quality. Normally what happens is quantize the model the quality decreases.
But what we have done what Google has done here is that they have applied QA on about 5,000 steps using probabilities from the non-quantized checkpoints as targets maintains the quality here. after QAT after quantization you can see that how the different models drop in sizes. So for example we have JAMA 27 billion parameter it goes from 54 to 14.1 GB in the integer for quantization. Now basically
you need a very good data set to train this model because it is definitely trained but not on your data set. Now for that we can go ahead and fine-tune this model. So for fine-tuning the model I am going to need some environments. Well first of all uh before finetuning we can see that um we can download this model from hugging face from Olama Kaggle LM studio and docker. You can use that. You can
try the models on vortex AI llama cpp jamma cpp. As a matter of fact it's so light that I can put it on my system as well. So if I sayama list you can see that I have this jama 3 270 million. Now I can say amama run jama 3 270 million and this will start up the model. Once it is done I'll say what is oneplus 9 and you can see that so fast what is the capital of Australia? See canbury so fast. So
let's go ahead and fine-tune this model. Now finetuning is a process which uh changes the weights. Now finetuning we would need a GPU. So my favorite is RunPods. You can go to this link which is get.runpods.io/p48. If you go to this link, you will get bonus uh from $5 to $500 when you spend your first $10 on runpods. So let's go ahead. Uh I am on my run pods. I have added some credits. I can go to
pods and start up one pod. So I go to pod and uh let's take a very small one. Let's take this 2008 which is uh let me increase the container size here to 50/50 and let me export 8,000 here and set overrite and you can go down and click on deploy on demand. This will deploy your GPU your RTX 208 GPU it's pulling and you can click on connect here and this will uh make your Jupyter lab ready. So
click on Jupyter Lab. Okay, it opens up the Jupyter lab. Now, the first and the foremost thing that you need to do is go to this terminal and update everything. So, a get update and a get upgrade-y. So, this will upgrade everything. Now, let's bring the notebook that I have prepared for you. Okay. So, the upgrade and update is done. Now, I have prepared this Python notebook, Jupyter notebook for
you so that you can test this out. I will share this notebook. Let's get started. The first thing that we need to do is to set up the development environment. Then prepare the data set. Uh fine-tune the model and then do the model testing. So the first and foremost thing is we need to do the installation. So to tensorboard transformers data set accelerate evaluate transformer reinforcement
learning library protobuff and sentence piece. So all these are installed here. Next we need to login to hugging face using the ATF token. So if I go to huggingface.co if I go to my profile and click on access tokens here and then I create a new token with the right permission and since I already created I will just invalidate and refresh this. So I get this access token here. Once I get that I'm
going to put this access token here somewhere. So the user data is not defined. So we just don't need the user data at all. I can just put in uh the hugging face token as this and just run this. Okay, so we are logged in. Then uh we don't need this part. What we're doing now is we are trying to load this base model of Jamaa 3270 million it is this model and then we are creating a checkpoint
directory inside my workplace workspace. So checkpoints and my jamma NPC and we are putting the learning rate of 5 e to the power minus 5. So we just set this parameters. Then we create and prepare the data set. For the data set we are going to use this mobile game NPC data set and you can see that here we have two subsets. One is a Martian and one is a vanish. There is a difference on how the
Martian and the vanish speaks. So for example, the Martian has 25 rows and when we say this player when hit it says um hello there then the alien which is a Martian replies in this format. So so we want to fine-tune this on this data set. So for example the Martian speaks with an accent that replaces the at sound with a z uses duh for duh this for this and additional occasionally clicks like kag.
This is uh a function which is created. Uh this function will extract from the sample the player. It extract the sample player and put it in this format. The NPC type we're using Martian here. We're loading the data set here. So this is the data set and then we're mapping to the create conversation here and we are putting it this format. So user role is user and then the content role is assistant
and then the contents. Then we're splitting the data set into 80 training samples and 20 test samples. And then we're printing this. So let's do this. Let's see if it's able to download the data set. So it's reading the data set. And you can see that we have the first example here. So content hello there, role is user and content is this uh and the role is assistant. So this is the data that we
want as an output or this is the data set that we want to train our model on. Now for training the model we need to first and foremost we need to have the model. So we have loaded the model using the auto model for causal lm which is a class of the transformers library. So we load the model using uh torch data type auto device map is auto attention implementation is eager. So we have this model
here and then we load the tokenizer as well that is used to tokenize the actual uh words into tokens and even detokenize as well. So if we print this we can see that we are going to load the model. So it's downloading the model and you can see that it's a very small size 536 MB only. It's uh loading it's downloading the model from the hugging face and you can see that we are in koda zero which is
just one GPU and torch.blat 16. We're using the bloat 16 here. This model is a bloat 16. Now before we do the finetuning let's test the model out. Let's test the output. The device is set to CUDA and if you print this question so I brought you a gift and the original answer is we need something like this. Uh this is how the Martian speaks but the generated answer is this which is very normal
answer. It's not even fine tuned as of now. So we want the output to look something like this instead of this uh beautiful output of the generated answer. What we can do is we can test out one more thing. So you are a game NPC then you can see that we get this output which is not at all uh desired. We wanted outputs that look something like this. Something how a Martian would speak. Now you might
think that if you could give a very good prompt. So you're a Martian NPC uses the X and replace S sound with a Z sound. And you think that you know giving a good system prompt could help. But again we have tested this out that uh given a very good prompt as well it's not so easy to get that output that you're looking for. The best approach would be to fine-tuning. So for fine-tuning we need to get
our arguments ready. So from the transform from the transformer reinforcement learning library we import the SFT config and we put all these parameters. So this is the directory where one is saved. This is the maximum length. Uh this is the number of epochs and you can see that we are saving at each checkpoint. Uh we are saving the uh eval checkpoint as well which we are going to use and plot
that. So we are going to push to hub as well. And these are the other different parameters. So we load up this arguments all the fine-tuning parameters. Now what you can do is we can start up the trainer. Now this trainer will use the model that we have already loaded the arguments which are these arguments fine-tuning arguments here. The train eval data set. So data set train that we have already
used uh and segregated to 80% training and 20% testing and then you're using the tokenizer as well. So we load this trainer. This trainer is now ready. Now what we can do is we can just say uh trainer.train train and this will train the model. So as you can see that's a pretty fast fine tuning. So you can see that it's saving the model. It's processing the files here. It's uh new data upload and
then it's saving the model. So that's done. Next what you can do is we can plot the the loss curve. So no module name and mattplot liv. We need we need a mattplot live installation. So pip install matt plotlive. Okay, that's done. Now let's try to plot this. So you can see that we have the training loss as here and we have the validation loss here. Uh what we did is we import the mattplot lib
library and then we get the access to the log history. Now for the trading losses we uh go through all the logs in the log history and if there is a log presence we take that log and we have that epoch. So we take all these and then we plot the train losses and the val the training loss and the validation loss. So as you can see there is some difference here. So what it means that when the
validation loss is greater than the training loss it means that it's overfeitting and in some cases we do need overfitting for example in this case in this case uh we want the model to talk like a Martian and for this we need to have some sort of overfitting so that it doesn't forget that it's a Martian. Okay. So we load this model again. We can load the model again and get some answers now. So
you can see that we are putting this question. Do you know any jokes? The original answer is this. And the generated answer is this. That's pretty cool. The question was this. The original answer is this. And the generated answer is this. And it's pretty cool. So it really works. And the next steps is you can push this model to the hub and use it on your own software and different applications
that you can think of. So this is how you can do the fine-tuning process. Go ahead and use the runpods using my link which is get.runpots.io/ /p48. So in summary in this video we have seen that Jama 3 270 million is an excellent model to fine-tune on your own data set. We have seen through the technical parameters we have seen how to fine-tune on your own data set and use it on multiple
applications that you can think of. Having said that congratulations on learning how to fine-tune. I will see you in this next video. Stay tuned. Stay subscribed. Have a nice life.

===== END TRANSCRIPT =====

===== BEGIN TRANSCRIPT =====
Video ID: tMZSo21cIPs
Language: en
Source: auto-generated
Saved At: 2025-08-17T11:39:19.793716
Google has quietly dropped their smallest member of Gemma 3 family which is this 270 million parameter model. This model is a latest open-source language model from Google which also comes in base template which means that you can fine-tune it on your own use case. This model is a surprisingly capable package built on the same foundational research as the Gemini models. It uses a transformer
architecture optimized for efficiency while maintaining decent performance for its size. In this video, we are going to install it locally and we will see how exactly it performs. This is Fad Miza and I welcome you to the channel. Please like the video and subscribe to the channel as that helps a lot. So before we start the installation, let's talk bit more about this model. The model was trained
on a massive 6 trillion tokens which is ironically more than some larger models in the family with a knowledge cutoff of August 2024. It supports a 32k token context window which is very large for a 270 million parameter model and comes in both pre-trained and instruction tuned variants. Now I will be talking bit more about its architecture later. But for now let's get the installation underway.
I'm going to use this Ubuntu system. So I'm going to use GPU for this video. But you can easily run it on CPU. And if it doesn't need GPU, it would just keep using the CPU. And you can see that I am using this Ubuntu system and my GPU card is Nvidia RTX A6000 with 48 GB of VRAM. If you're looking to rent a GPU on very affordable price, you can find the link to master compute in videos description
with a discount coupon code of 50% for range of GPUs. Let me now install all the prerequisites and they include to transformers and lot of other things. While that happens, let me also introduce you to the sponsors of the video who are Agent, which is the world's first multi- aent workforce desktop application, empowering you to build, manage, and deploy a custom AI workforce that can turn your
most complex workflows into automated tasks. Now, one last thing which we need to install and make sure is this transformers version. Make sure that you're installing this branch which is also present on their model card because this is the branch where this Jamma 3's newest 270 million parameter is supported. Also, this is a gated model which means that you would need to go to hugging face log in
and accept the terms and condition and then you would need to grab your re token and log into hugging face which I will also show you shortly. If you don't do that, you won't be able to download this model. By the way, this model is also available from Kaggle, uh from Google's Vert.ex CI and also from Google's AI studio if you are interested. And now I'm going to log into the hugging face. So I'm
just using this hugging face CLI. And now you can see that this has been deprecated. I didn't see it like few hours ago. So it seems they have just deprecated it. And the new command to use it I believe is this uh H. Sorry, I'll just go up. HF O login. So let's do this HF O login and there is no warning now. So things keep changing and we are working on the bleeding edge. So this will happen
almost every day. Anyway, I'm going to paste my read token which is a pre token from hugging face uh profile. Also it's always a good idea to set your token like this in environment variable. So let me set that and clear the screen. And now I'm going to launch my Jupiter notebook. First thing we need to do is to download the model and its tokenizer. So let's do that. And the model is now loaded.
It's a very quick model. Just over 500 meg of size. And if I quickly look at the VRAM consumption, it is not even using my VRAM. It is all on CPU. And now let's do the inference widget. We are going to test it out on various uh prompts. So first up I'm asking it to write me a short short poem about AI. And then there are some hyperparameters. Input is being converted into these numerical
representation which model can understand. And then model is generating the output. Tokenizer is decoding it and we are printing it out. So let me run this. While it runs let's check the VM consumption. Nothing there and there is your poem which is not bad at all. The language is coherent. It has followed the prompt and has produced what we asked it to produce. Okay, let's try out a bit more
stuff. So next up I'm just checking its language proess. So I'm just going to make it like this. So I'm asking it who this following quotation belong to. It is different. It is difficult to get a man to understand something when his salary depends upon is not understanding it. So let's run this. There you go. So it says Mr. Smith which is totally wrong. I believe that is uh Upton Sinclair I
believe. Anyway, so I don't think so it has done good. Anyway, let's check out bit of a multilinguality. I will keep it simple. I understand the limitation of the model. So, I'm asking it to write me the number from 1 to 10 in Indonesian language in form of English words. Even if it produces it in Indonesian, I'll be fine. But it doesn't. Okay, that is interesting. So, it is not really that
multilingual. The model card says 140 languages. Anyway, I'm going to stop teasing the model. I will maybe keep it bit different. I will make it bit generic language task this time. I'm asking if birds have wings they say. But of course they have no way of verifying this. If you were a bird, how would you verify this? Let's try this out. Okay. So you see I will even if I increase this to 56. Let's
see what model says. And by the way, this is not a base model. This is an instruction model. It says I'm not sure how to verify this statement. It's a common misconception. Okay. Interesting. Okay. Let's try to be fair with the model given its size. So I will just be going with some basic factual questions, simple explanations, maybe some short creative tasks and elementary math, that sort of
stuff. We already have tested its capabilities in writing that poem which was quite good. Let's see if it can write this short story about a cat who discovers a hidden door. And I want the model to keep it 100 words. So let's check it out. There you go. Whiskers twitching Clementine the cat. Very nice. It has given the cat a name and then a breed. It is explaining the cat exploring the attic. dust
pore danced in the single shaft of sunlight illuminating a forgotten trunk. She sniffed the air. You know what? I think this is quite good that it was safe. So, it has really done good there. I think for a 270 million parameter model, that is quite a good response. Okay, next up, let's see if it can summarize this paragraph. So I'm asking it to summarize this whole paragraph in one sentence that
machine learning is a subset of AI that does this and this and there you go. I don't think so it has done the summary so it cannot do summary. You see maybe I will run it one more time. Noops it cannot do summary but it is hallucinating and printing some Russian here. Interesting. Let's try out a coding one. And I'm just asking it to write me a simple Python function to check if a number is even
or odd for simple task. And I think it does a decent job. But I think co's model were maybe slightly better than this in similar size. Maybe they were just bit higher around 500. So it is even half of its size. Anyway, let's see if it can write this simple code or not. Taking bit of a time. Let's check the VM consumption. Nope. All on CPU. There you go. So, our function is there. I'm quickly
checking. Yep, looks good. Very nice. Not only that, it has also explained it and then given us the proper use. I like it. I think for this size of model, this is a very good response. Let's ask this question. The correct answer is 20. That if I have 15 apples and give away seven and then buy 12 more, how many apples do I have? As I said, 20 is the correct answer. Yep, you see 20 apples. Very
nice. And let's check out this one. Let's run this. The answer is very simple. 40 for this geometrical uh question where we are asking it to give give us the area of a rectangle and the answer is correct. Let's check out one more multilingual one and then we will call it a day. So I am asking it to say good morning in 10 different languages and I'm asking it to make sure you know those languages.
Not really. I don't think so it is multilingual. I think there are a lot of limitations in this 270 million version but does a decent job for many of the basic tasks in math, English and coding. Let me know what do you think. Please like the video and subscribe to the channel as that helps a lot. Thank you for all the

===== END TRANSCRIPT =====

===== BEGIN TRANSCRIPT =====
Video ID: Sp4qE3jDi0M
Language: en
Source: auto-generated
Saved At: 2025-08-17T11:39:20.996530
Google just launched one of the tiniest Gemma model, a 270 million parameter, not billion parameter model that you can run on your computer. In fact, you can run on your phone just like on the screen. So, this is a new model that is ideally designed for better fine-tuning. So, you can use this model and then fine-tune on things that you want. This is supposed to be hyperefficient model. That means
that even when they tested it on Pixel 9 Pro, this model took only less than 1% of battery for 25 conversation. So that is a really power efficient Gemma model that you can run local. In this video, we're going to discuss everything about Gemma 3 270 million parameter model and we are also going to run the model on Olama. I'm not going to go dive deeper into the benchmark because when I tested the
model, the model looked pretty average. But again the problem right now is our brain is so occupied with the GPD5s of the world claw four opus of world. So when you have a 270 million parameter model my unconscious bias might strike in and then say the model is not really good. I don't see a point in discussing a lot of benchmarks. But if you were to see one thing that Google has mentioned here,
you can see Gemma 270 million parameter model is somewhere here in terms of model size. But it is doing better than you know like like the other um models in this particular family. You can see uh even better than the 500 million parameter model from Quen 2.5. I mean I have a good opinion about Quen all the time. So I'm not going to take the benchmark as it is. But the main thing here is that this
is this is a model that should be appreciated for a variety of reasons. First of all, the model comes with 270 million parameters and the model itself comes with 256,000 tokens. Uh so there is a large vocabulary. That means the model can do a lot of things. The large vocabulary also enables you to do finetuning. When you do finetuning, the base knowledge is within the model. So that means the
model uh it becomes a good candidate for you to do a lot of finetuning both on domains and also languages. They've also released an instruction tued model. So that means that uh you can use this model to invoke a certain instruction following task. So the model is not designed for complex conversation obviously 270 million parameter model but the model knows to follow instructions right out of the
box. I think for me the main thing here is that the model can run on your devices like pocket devices, Android devices and there are like a certain easy ways for you to run the model within your smartphone like especially Android phones locally without sweating and the model seems to be running good maybe not like the highest accurate chbd level knowledge but the model works like that's the first
step I would say in short the model has been designed for task specific fine-tuning and strong instruction following and text structuring capability These are already trained and it's part of the pre-trained base model in itself. If you ask me when I should use the Jamaa 3 270 million parameter model, I would say like the first thing that should come to your mind is do you have a scenario where
you are on an edge device or some kind of a compute crunch? Then this is one of the models that you should turn to and then start using the model. So for example, you've got a bunch of Raspberry Pi clusters. So you want to run this model for a very simple task in a in a closed network then you can easily use this model. So whenever you want to respect privacy whenever you want to have specific
task uh related fleet of models then you can have the fine-tuned model run it on Raspberry Pi run it on different edge devices. So this model is extremely extremely useful for it. The other case is like if you have something where the task is very well defined, very well structured like for example writing a story or some kind of text processing like summarization um give me a tweet that sort of
thing. I think that is also a place where you can use this model and mainly if you're a developer I would say try fine-tuning this model. I might put out a separate tutorial on how you can fine-tune the model. So they've got a demo where they're using transformer.js js which will download the model locally in v was web assembly language and then you can run this model. So as you can see here this
has been downloaded. I'm going to switch off my internet just just to show you like this can work without internet. I'm going to go here and then open hacker news and it won't work. I can just play the dino game. So I'm going to go back here. I've already run downloaded the model. So you can see here I've already downloaded the model. I'm going to select a bunch of characters. I'm going to select
a sleepy astronaut and uh under the sea um fairy tale short make my story and the moment I click make my story you can see the story is being populated here and you know you would see a coherent story I mean it's not a story that that's going to win I don't know what kind of award like pulitzer Emmys Emma I don't know what kind of awards you have got but the point here is that a 270 million
parameter model that runs locally within your browser without even having to have internet like I still don't have internet as you can see here I'm not like kidding I can play dino again so this is a really good model for specific task and I don't think this is even fine-tuned in this particular case you can just go ahead and then start using the model for specific task and in my phone demo the
model actually has done a decent job not like a great job but a decent job I used Google AI edge the gallery app and then just downloaded the quantized version model from hugging face and then started using the model and it is doing a decent enough job. So let me show you how you can download this model. The first thing that you have to do is you have to say just Olama run gemma 3 270 million
parameter model. The moment you save this Olama is going to pull the model into your computer. So the model is available for you to use it. Going to just run this. In my case the model has been already downloaded. So I can just simply go ahead and then chat with the model. write a short story on Elon Musk falling in love with Martians. This is a 2019 laptop. It's like 6 years old. I mean, it's not
a very bad laptop. I still have got a decent RAM like 32GB, but still the speed in which the model generates like coherent text is amazing. Like their friendship blossomed, a testament to the enduring power of human connection, boundless potential of the universe. So this also being a smaller model one thing that you have to keep in mind is if you're going to use quantiz version like for example
when I use the same model on my smartphone it doesn't have the same quality that I can try on Ola. So you might have to keep that in mind like whenever you are using different quantizations or different versions of the model it may not be as good as something that you're using without any sort of quantization there. So so like I can ask another question. write a Python program to make a bar chart
um in seaborn. Definitely not a question that a lot of people are going to ask LLMs but if you are going to ask LLM then the model is doing a good enough job. So there is like the markdown tag then you've got import cours SNS and um where is it making a bar chart. Okay, it's trying to make a bar chart. I hope this code actually works and um yeah that's it. So, this is a model that is not going to
blow your mind, but this is a model that we all should appreciate Google for releasing. And um I love this model. Um I'm going to probably have a fine tuning tutorial if you're interested in it. Subscribe to the channel. See you another video. Happy prompting.

===== END TRANSCRIPT =====

===== BEGIN TRANSCRIPT =====
Video ID: uzyOnqJDcaU
Language: en
Source: auto-generated
Saved At: 2025-08-17T11:39:22.070765
Hey, hello everyone. Welcome back to my channel. Today I'm going to unpack Gimma 3 which is the latest edition of Google one and it's highly compact and hyper efficient Chennai model and let's understand what is the crazy things uh Gimma 3 is going to comes up and how it's been technically been compact and how it's going to be trained and how it's been fine shun to have a task specific fine-tuning
purpose and in this session I'm going to unpack most of the things uh how it's been uh what is the technical stack how the embedding and token vocabulary and how it's been powered and later on I'm going to show you how it's going to working in kind of like a real time or finetuning ones and I hope you are going to watch this entire one to understand a gimma tree and you are an expert card or you
wanted to be play with um large language models on a devices which is like a mobile phones and cam uh tree is perfect for you. The reason is right is a hyper efficient energy usage. It means like it's optimized for your battery um usage on your phone and it's designed for task specific fine tuning. I'm going to discuss into that as well. And it has a strong instruction following capabilities and
it's a compact because it has a 270 parameters total 170 million embedding and plus 100 transformer blocks and as I said right this is innovation cost of battery life. So when you are doing interacting with your large language models uh the AI powered mobile ones which is required higher battery efficiency and gimma 3 is been innovation for the cost of the battery life. So per 25 compositions
which only cost 0.75% of the battery. That's amazing. And the reason why I said that because when I'm interacting with my mobile phone and I'm continuously typing it, it consume my uh battery usage and but Kim 3 with having that instruction following it's out of the box and it has a quad checkpoint available and I'm going to talk that as well into the performance of the ones and in architecture
and the design of this it has 270 million parameter with optimized distribution with 170 million and 100 transformer. And if you look at like the in terms of the small large language model, Gimma 3 sits on top of a I love the small models because it's perform uh much better in kind of like a faster responses and I can use as a my prime rooting response LM when I'm going to define certain kind of
like a um addition making on the logger LMS which having a deeper analyzing skill set and large vocabulary. It has a 256 token vocabulary handling specific Andrea tokens. It's amazing. You you can you you can use a gamma tree and fine tune and it's going to responding into the different languages and the performance wise it's best in the size of the ones and this model is available in pre-train
and instruction based. It's very good for a developer specific like me who wanted it to be pre-training my model but I don't wanted to spend a huge amount on a GPUs to train my model. Gimma 3 is perfect for those who wanted to play with specialized applications which I'm going to be show how you can train a specialized application with lesser compute cost for you and it's going to have the AI
capabilities to that. So it's ideal for sentimental analysis and entity extraction, query routting and text processing and compliance check. That's what I said like it's best for your query rooting. a gamer going to be your first point of interaction and then it's it started going for the launch language model and it's very fast in terms of the response to the user and it because it drastically
reduce your interference cost and it deliver the faster responses and rapid uh kind of like a deployment all you need to do is like it takes I tried with the finetuning of the gamer 3 it takes me less than 1 hour to pre-train memory model and started working on that and it's it takes lesser time lesser compute for you to fine-tune and when you are using into the privacy is a major concern and with
handling the sensitive information without sending the data into the cloud then KA 3 is a perfect and it has a specialized multitask capabilities with the budget in consideration and it's available to download this model from hugging face Olama and Kegel LM studio and docker. So it's it's a latest version. It can run on your local or might be cloud run and you can try this model from gamma tree
and uh MLX and you can finetune this model with unslot jacks and hugging face and it's easy to apply because it's easy to finetune this model. Let me go ahead and try to unpack this entire model and let's understand more about this model. So as I said like it's a compact in terms of like the model wise extremely best for the energy efficient following instruction fine tuning. Let's understand what
is quantization aware training and checkpoints are available. So if see the gamma 3 is like quad is dramatically reduce the memory. The memory is a major thing. If you see the deepse R1 and you know chartboard error the gamma tree is going to be into the perform best but it's a major thing is like the performance is like reduce the compute power. It maintain the quality right and quantizing the
model fully trying. So it means like it's take lesser CPU and it's going to be give the faster responses and gimma 3 with 27 billion parameter stick 41 and gimma 3 1 billion parameters take 0.5 GB. So this latest edition can run in your mobile phones which is going to gives you perfect for uh giving the high um responses for your queries and I I personally believes that it is going to be best in a
business if you wanted to use a query one and now I'm going to show you how to create a one kind of like a bedtime story. So I built this app where I given uh betam story generator and all I needs to do is a magical cat and I can write my own and I see is like under C and let's meet uh uncover secret door and it should be bedtime and funny and all I need to be write it the story. So it's it is so
fast and quick right it's understand the entire context of what I'm going to writing a story and it's going to come so the ginger tabby a perpetually surprise expression was bored the sand was sinking below the waves so it has given me the context what I'm looking and it's giving me the entire betm story and writing once I'm sharing this into the links and you can start playing with that with
you're giving your own uh prompt for that and the fine-tuning of this model is pretty um amazing right I'm going to walk through my upcoming video how I'm going to fine-tune this entire model and how it's going to come up with a different language to do it and in terms of like it's going to available into hugging face you can start uh looking up this model I highly recommended you to go and try to
see this model and how it's going to support 140 languages which is available and go and try this model and let me know what you're trying to build it. On my upcoming video I'm going to show you how I'm going to fine-tune this model. Thanks for watching.

===== END TRANSCRIPT =====

===== BEGIN TRANSCRIPT =====
Video ID: YPz7khn2us4
Language: en
Source: auto-generated
Saved At: 2025-08-17T11:39:23.130722
Google has just launched a brand new AI model and it's tiny, super fast, and so efficient that you can run it on your phone without worrying about your battery dying. This is the new Gemma 3270M, the smallest model in the JMA family. But don't let the size fool you. It's smart, powerful, and ready for some really cool uses. So, in this video, I'm going to break it all down for you in simple terms
so you know exactly what's new and why it matters. All right, before we dive into all the technical stuff, let me quickly tell you what this GMA 3 270M actually is. Okay, Google officially released it on August 14, 2025 as part of their GMA 3 model family. Now this family has different sizes 270 million, 1 billion, 4 billion, 12 billion and 27 billion parameters. Think of parameters like brain
cells of an AI. More parameters usually mean a bigger brain, but also more power needed to run it. The 270 million version is the smallest one in the family, which makes it super light and fast. This version is text only, meaning it works only with words. It can't process images or other media. Google designed it for low cost, fast and private tasks. So if you need something that can run directly
on your laptop or even on a phone without using tons of cloud power, this is perfect. You can already find and try it in multiple places like Google's Vert.Ex AI model, Garden, Hugging Face, Olama, and other platforms. So whether you are a developer, hobbyist or just curious, it's very easy to get your hands on it and start experimenting. Okay. Now let's talk about what's actually inside JMA 3270.
First, the name itself tells you a lot. 270 means it has 270 million parameters. You can think of parameters as the little switches inside an EI's brain that help it learn and respond. Out of these about 170 million are used just for understanding and storing words and the remaining 100 million are used for processing and reasoning. Next is the context window. This is basically means how much text
the model can read, remember and use at one time. The 270 million version can handle 32,000 tokens. Tokens are just chunks of words. For example, a whole sentence might be 10 to 15 tokens. The bigger Zimma model can handle 1 lakh 28,000 tokens, but 32K is still plenty for most task. This model understands over 140 languages. So, it's not just for English. You can use it in Hindi, Spanish, French
and many more. And the important point is that it's text only. So, unlike some bigger AI models, it can't process pictures, videos or audio. It works purely with words. Another cool thing is how it was trained. Even though it's small, Google fed it a massive amount of data around 6 trillion tokens during training. That's like giving it a huge library to learn from, which is why it can still
perform really well. And here is why it's so good for phones and small devices. It doesn't need much memory to run. In its normal form, it takes about 400 MB of memory. And if you use spatial compression called 4bit quantization, it can go down to just 240 MB. That's tiny for an AI model, which is why it can run on devices like laptops, as better PI or even smartphones without burning your
battery. Now, let's talk about why this tiny model is actually a big deal in the real world. Google has made the JMA 327M super efficient by using something called INT4 quantization. I know that sounds technical, but here is the simple version. It's like shrinking the model's brain data into a smaller, lighter form without making it forget how to think. This means it needs less power, less memory,
and can run on small devices really well. In Google's own test, they ran this model on a Pixel 9 Pro smartphone and had 25 different conversations with it. And you know how much battery it used? Just 0.75%. That's less than 1% for all those charts, which is crazy efficient for an EI model. Because it's so light, it also starts up faster and responds quicker than big EI models. You don't have to
wait for it to ver up. It's ready almost instantly. Now, if you're someone who likes to customize things or build your own UI projects, JMA 327TM is very developer friendly, Google has released it in two readytouse forms, pre-trained and instruction tuned. The pre-trained version is like a blank but knowledgeable brain. It knows a lot, but you have to guide it. The instruction tuned version has
already been trained to follow your instructions more naturally. So it's ready for tasks right out of the box. If you want to fine-tune it, which means adjusting it for your specific needs, you can do that on popular platforms like hugging face unsolved or jax. And when it comes to actually running the model that's called interface, it works with a bunch of tools. Lama.cpp, jamma.cpp, CPP, light
RD, MLX and more. These are just lightweight programs that help the model run on different devices efficiently. So now let's talk about how does these 270M version compared to the other ZMA models. Let's break it down. So in the ZMA 3 family, there are bigger versions like 1 billion, 4 billion, 12 billion and 27 billion. Those numbers mean they have way more parameters. So they are more powerful
but they also need a lot more memory processing power and energy to run. The 270m is the smallest which makes it much lighter and faster. Another big difference is this is the only text only model in the lineup. All the bigger ones are multimodel meaning they can handle not just text but also images and maybe other types of data. So if you need something for both pictures and text you would pick a
bigger model. But if you just need text tasks, 270m is more than enough. It also has a smallest context window. Uh 32k tokens compared to 1 lakh 28,000 in the bigger ones. That means it can't remember as much in one go. But for most everyday tasks, 32k is still plenty. Now let's see what people in the tech and AI community are saying about JMA 327m. On hacker news, a lot of users are impressed by
how small this model is. One person even said it's small enough to fit almost anywhere and easy to fine-tune, which basically means you can run it on all kinds of devices and still customize it for your own needs without much hassle. on Reddit. People were surprised that even though this is the smallest JMA model, Google still trained it with a huge amount of data around 6 trillion tokens. That's
the kind of training you would expect for much bigger models. So, it explains why 270 million still performs well despite its size. Overall, the general feeling from the community is very positive, especially when it comes uh to its uh battery efficiency and low computing needs. People like the fact that it can run locally, save power, and still handle many practical tasks without relying on big
cloud servers. So to wrap it up, I think Gemma 3270M is a big deal, especially for ondevice AI. Usually AI models are huge and need powerful cloud servers to run. But this one is small enough to live right on your phone, laptop, or even tiny devices. What I really like is that Google isn't just chasing bigger and bigger models. They're also focusing on efficiency. And in the future, I think we
will see more AI tools that are not only smart, but also light, fast, and battery friendly, just like this one. And that's everything you need to know about the new Jama 3270M. Small size but with huge potency. I'm really curious. Uh would you use this on your phone, your laptop, or maybe even on a tiny device? Tell me in the comments. I would love to hear your ideas. If you found this breakdown
helpful, make sure to give this video a like, share it with your friends who love tech, and subscribe so you don't miss more updates like this

===== END TRANSCRIPT =====

