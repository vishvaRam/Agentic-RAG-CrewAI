===== BEGIN TRANSCRIPT =====
Video ID: --Wk3LgrMjo
Language: en
Source: auto-generated
Saved At: 2025-08-17T12:07:10.109699
Hi. So, we're going to get one 2GP running on RunPod and use multi-alk to generate a animated video. First, in your runpod, check whether you want secure cloud or community cloud. And then, this is really important. Make sure you filter your CUDA version to 12.8 so you do not hit issues with compatibility on versions later. Then, you're going to want to select your machine. I've been going for
A40s recently. It's a good balance of price, availability, and RAM. Um, you're going to change your template that you use to the RunPod PyTorch 2.8. And then you're going to edit the template to increase your disc storage to 60GB and to expose port 7860. When that's done, make sure you're on a GPU count of one so you're not spending on what you don't need. And then select whether you want on
demand or spot. I'm going to select on demand so I do not get interrupted. Then hit deploy. It's going to take a little bit to get your machine going. Once it's ready, you can hit that connect button. You can use the web terminal through RunPod or SSH if you're more comfortable and prefer to do that. But for this, I'm just going to use their web terminal. You're going to cd into workspace. Then
you're going to clone our repo. I'm going to share this full document so you can copy and paste um like I did the exact commands. So clone the repo. You're going to cd into it when it's done. And then we're going to check out an exact commit so that we know that everything will still work as intended even if there's changes later to the repo. Once we've done that, we can install our requirements.
I sped that up a bit since it takes a while. And we're also going to install ffmpeg which is happening now with a separate command. And then this is very important. You want to install this exact version of gradio. You're going to hit issues if you use a different version of gradio. And then you're going to install save tension. Great. And then this is optional, but we're going to call T-Mox so we
can attach back to our terminal we get shut out and then start our T-Mox session. And now we are ready to kick off our server. It's going to take a little bit, but then you're finally going to see that it's running on local URL and you can connect right through that through your runpod terminal. And there we have it. We have the the app running and we're going to select the vase multi-alk. There's
both vase multi-talk options and regular multi-alk. I've had great success with the base multi- talk. Great. Uh we're going to use a reference image and we're going to inject the people and the objects from the image. So, we're going to drag that right in. And then we can want to keep the background. you have the option to do either. And we have two people speaking. So, we're going to try the
autosparation option, which is a nice convenience feature, but it's not I found not the most stable. So, I'm going to drag in one audio that has both voices. The character on the left is speaking first. Um, that's that's what the model expects. And I've had mixed success in the model knowing who is speaking or not. And sometimes you get both characters uh lip-syncing the lines. So it can be a
little finicky. In the prompt, I tend to tell it who is speaking first. I don't know if that actually helps, but it's worth trying. So I'm just giving a prompt to polar bear and a nerdy man or at a whiteboard having a conversation. The polar bear excitedly teaches the nerdy man and the polar bear speaks first. And then just for funsies, I say make sure the lip sync is very accurate. Want to change
our dimensions. I want this tick tock vertical size. And then we need to calculate our number of frames. It should be the total seconds of our audio times 25. So, we put that in. I'm going to keep the number of inference steps at the default 10. And I'm going to add tcash to speed this up. Um, it's recommended that you start the tcash at 10% of generation, but I'm just going to do it for the whole
thing. Then check our inputs and click generate. You can see that it'll be using a sliding window since we go beyond the uh the max in one session, but that's great because it'll stitch the whole video together for you. So, we see that we're downloading the models. This will take a bit, but it only happens the first time you um generate Great. We can see our resource utilization back in the run
pod terminal. See, they're not using our GPUs yet. And we hit an error with the separating of the audio. So, we're going to do that ourselves by setting the two speakers option instead. What's nice is this interface has a little editor where you can trim the voices. So, I'm just going to use the same audio twice and trim so that we have the first speaker on top. Let's check to make sure it's
correct. Great. Trim that. And then I'm just going to drag the same audio for the second speaker and trim the second speaker part. Now we can keep everything else the same. Oh, we need to change our frames because we made this a little shorter. 7 * 25 should be our frames. And then we generate again. And there we see that it is going. And I accelerate the speed for the demo purposes. It's going to
take a few minutes to generate the whole thing. What's nice is when the first sliding window is done, it lets you see what's done so far. So we can see our first 5 seconds. Then save that in in case something happens on the second window. And there we have it. We have our full 6 seconds video generated in 612 seconds. But close. You reward the model when it does a good job. Good model. Here's a
snage. Uh, thanks for watching.

===== END TRANSCRIPT =====

===== BEGIN TRANSCRIPT =====
Video ID: _9kYAOrK3NA
Language: en
Source: auto-generated
Saved At: 2025-08-17T12:07:11.340583
Hey, how it's going guys? In this video, I'm going to show you how you can deploy open-source language models on Rank AI. Run AI is a cloud computing platform which is more affordable than RunPower, right? And they are reliable as well. So, if you are building your AI infrastructure, Rank Aai might be a good choice. Let me show you that how we can deploy language models or different types of AI
models on Rank AI. So if you look at here on my screen, I'm already on their console, but just want to show you a bit of pricing here, right? So if you come, you know, on Rankai, you can find out the differences uh that they provide and with uh run power. So if you look at here on the screen, right? Uh best prices for 10,000 plus GPUs and they have different types of GPU machines that you can find
it out over here. If you look at the head-to-head pricing with Lambda, Run Power, Pure GPU, Rankai is like really good in that regard, right? And they already have you know images for you. So if you are if you want to work with let's say image generation or media generation kind of models you can use comfy UI or stable diffusion web UI right in that regard. And you know if you want to work with
best of open source language models you know you can we can use deepseek and llama right and that's what we're going to do in this video. So the moment you sign in, you get uh $5 uh for use. So test it out. And that's a good thing because we can just play around and see if that you know if this this is makes this makes sense or not. Right? So if you look at here on the on the dashboard once I log
in the console in the left hand side the design is really minimal, right? Most of the time when we work with cloud computing platform, you know, it's like really difficult to kind of navigate different things, right? The features that they provide. But on runki uh I really like the name as you know they say it's reliable run cleaver cloud computing reliable cleaver and I think that's where we are
moving right nowadays when uh the way we are seeing the explosion in the aentki uh field as well. Now here on the left hand side we have instance and storage and you know image and some other utilities like settings and whatnot. If you go to bill, you will find out that I already have my, you know, $5 uh that I already got after, you know, logging in. You can see 5 USD that I have it. Now, there
are two things you can do. One is instance and the other is image. It says you do not have created any images yet. So, if you click on create, you say select instance and we don't have anything over there, right? Because we haven't created any instance, you know, in that regard. So, first thing that we have to do, you know, we have to go to instance. So let's come to instance and then click on
deploy. Now here you can select different types of GPU machines based on what you want to work with. If you want to work with PyTorch, of course that is recommended, right? When you're work working with large language models, you can again you can work with TensorFlow if you want and you can also create an environment with Micond right you can also do that. Now if you look at here we have GPU
models and RTX 490 is available as of now. you know the A00 and H00s are sold out of course because those are high uh you know highly required GPUs for training models or fine-tuning models right now you can see we have RTX 4090 it says 24 gigs 24 GB of VRAM uh that we have over here 16 CPU cores 100 GB of SSD and 64 GB of RAM and if you want higher you can get these things but this is the uh list
that we can have over here you can see uh it says this total of this uh that we have price per hour right now you have the container image you have system image in the container image you have PyTorch uh and you can also go with CUDA as well and if you want we have deepseek you can see see it over here deepse image it says view the readme if you click on the readme it takes you to their
documentation it says what is VLM deepseeek R1 distill you can see it says knowledge distillation I hope you understand about the distillation techniques that's how deepseek was like you know created uh where they use distillation methods to kind of distill the knowledge from best of the models right chropics geminis and so on and so forth and they created a distilled model that's named deepseek
within the deepse as well they had different types of you know distillations that they did with qn models and llama's model and so on and so forth now you can find it out over here it says uh covering 1.5 to blah blah blah we can go with qn 1.5 you can see rtx 4090 the advant advantages this are a lot you know it'll be more greener as well right you can see that they have and we can just copy this
and run it guys you can see it says the preconfigured environment requires no additional setup so the good thing about run pod or runai or any other right and I will say runai because they're affordable as well the great thing about them is that you don't have to handle the infrastructure by yourself runkai handles the infrastructure for you right and that's what good about it and you can see it
says Preconfigured environment requires no additional setup. Launch the service with a single command. After initializing your instance, execute the following command in Jupyter lab. You can see start VM API service. And they also have given an example. That's what we're going to do. Basically that is exact same thing. Right now you can see it says recommended GPU configuration for this model. If
you want a higher model like 7 billion parameters or llama 8 billion parameters, you need, you know, different types of machines. You know, you might need two RTX, you know, four RTX, 8 RTX, depending on what you want to do. Now, okay, we'll we'll come to this. The first thing that we're going to do is of course going to take this machine here. Let's select DeepSc. And I think you can give a name.
I'm going to call it Deepseek demo or something. Let's do it like that. Uh you need to create and you can create it here. Uh you can see uh storage of course we need a storage to kind of create it. Uh of course all the all our yeah I think this would be fine. I'm going to say I think I can just call it AI anytime. Uh this is our bucket where you going to have all our you know files and everything.
And you can see we have AI anytime that that is there. You know you can upload your stuff data and whatever. Right now I think this looks good. And if you go back instance I'll show you is we have to create it in instance. Right. And you can see we have this. Let's click on mount here. And in the object storage and in the instance I'm going to just call it deepse demo as I selected it earlier. And
we're going to click on deepsek here. And you can see the pricing. Everything looks good. And click on deploy on demand. It it takes a bit of time. You can see the selected image VLM DeepSc R1 digital. Right. So VLM is just some kind of decoding techniques if you if you learn about it. So that makes your inference really fast and of course some compute limited devices that's crazy right. So you
can have a look at VLM framework as well guys. I have a couple of videos on that now when you click on deploy on demand right it's going to take a bit of time for you. You can see the status initialize right and once you have everything set up you can just click on start and you can just go ahead and start using you know the deepsek model you know sometimes what happens guys right let's say we're
working with collab pro or any kind of uh or collab in general or kaggle right you might not have sufficient uh you know credits to work with and let's say if you quickly want to try it out and build some demos and you need GPU you can just sign name in the console on Rank AI because they give you $5, right? And that's why I wanted to try it out and see you know if that is really making sense. So
in future I'll be working more with Rank AI rather than working with runpod or let's say you know uh pure GPU or lambda or you know AI or any other thing right because I can just focus on this this cloud right and it gives me really affordable pricing to kind of spin up models. So, keep that in mind. If you are struggling with collab credits, you don't have collab pro, you can just sign in here
and get $5 and try it out. And of course, you can add more. I'll come back once this is done, guys. Fantastic, guys. As you can see, uh, that our instance is running now, right on rank AI cloud here. And you can see the product specification. We had an RTX 4090 with 24 gigs of VRAM. And then you can find out of course the IP addresses and how can how you can connect with your external systems
right you have to do SSH and you connect with it and the beauty of this is Jupyter lab and the data scientists like us right we want to work with labs right the notebooks and where we can use this let me show you how how it works right so you have to click on Jupyter lab I already have done that just to save some time right now you can see the moment you do Jupyter lab you will see this lab and of
course you have to create a file Of course, you can click on this plus, right? When you click on this plus, you'll find out the launcher. And I assume you guys know about launchers or the Jupyter Labs, right? Basically an uh collaborative and interactive IDE for data scientist. Now, if you look at here, we have Python 3 notebooks. I created a notebook here called model.ipb and where I surf the
model through VLM. And guys, you know, VLM is fantastic, right? If you go on VLM, you can see a high throughput and memory efficient framework, you know, for inferencing and serving large language models. It's very easy to kind of serve models as an API endpoint, you know, through VLM. Now, if you look at here, you have to do VLM serve and of course the path of the model. So the model path that is
coming from hugging face. So you can see the model hugging face deepseek AI and then the model that we are running, we're running a QN7B. We are running the distilled version you know of DC Q and 7B on a 24 gigs of you know virtual machine on run AI through VLM. How cool is that? Right now you don't need a high compute to kind of work with open source model. That's crazy right and it runs on port
8,000 because that's what we are forwarding right on port 8,000 and of course has a model length of 65,000 as a context window. As you can see we if you scroll down you can find out here right it runs on 8,000. You can see uvicon running on so socket 8000. When you are running this command for the first time, it's going to take a bit of time to you know load the model. You can see it's it says
loading safe tensors checkpoint s right? You have to get all the sarded weights and load it into this memory and that's how it tells you that okay now it's up and running on port 8,000. The moment it's up and running on port 8,000 you can basically use this as an API endpoint. Now once that is running on 8,000 you can use this wherever you want right within the same environment or you can also
call this externally right if you want to call it. Now uh the good thing is that they have a sample test.py if you click on this test.py Pi you can see I already have it over here it's compatible with open AI SDKs right we can use a model that we have served through VLM via open AI SDKs right because they're compatible now you can see we are doing from open AAI import open AAI we have our API key
base URL and it runs on 8,000 you know version one this model name will fetch up the model name you can you can also fill the model if you're running multiple models and you can find out uh some kind of very simple prompt template over here and the temperature you can reduce that and I have clicked you can click on this plus icon again the new launcher and get a terminal you can see my terminal
terminal is still running here and I already have done python test.py now in this python test.py You can see it says think because it's a reasoning model right it has reasoning. Now in the reasoning you can find out all right I just received a query asking me to write a composition of 200 words blah blah blah right that's what we have right a composition of 200 words now here you can do anything
you want guys you know it's up to you so let's say for example you know you want to change this you want to say write a fast API endpoint to to fetch uh weather API or weather details from an API or something like this right uh you know if you save this you run this again this will give you a similar response you do python test.py pi and it's going to give you some kind of output and that's what
it does. Now how easy it is imagine even if you are not a not an AI engineer right you are not a pro in this field you can still work with it you can still work with open source model and this kind of you know uh innovation is really important uh from an infrastructure standpoint where you're working with sensitive data where you have you can create your own you know infrastructure your own you
can work with your own virtual machines and then you can you know spin up these uh images like deep comfy UI or SDS and llamas and so on and so forth and keep all your data private within the same infrastructure. So you can build onremise solutions right using run AI uh cloud computing platform that we have been you can see that we got our uh output here right uh such a beautiful output I really
liked it guys and you can see it gives a fast API endpoint you know to fetch weather details using an external API of course they're using open weather map API that gives you free credits to fetch weather details now this is fantastic and not only this using this you can you know you can use different models as well not only the same model you can different types of models and that's how it's
working. uh I'm going to create you know a few more videos on this because I want to show you that how you can use this uh basically this running instance in an external applications like let's say you have an application outside this environment right on stream or or a react front end or or an interface how do we use this right and you can see we have our IP addresses and SSH login of course I'm
going to delete this because I want to save the credits now fantastic you can see the storage over here we have the we have the instance over here where we running our instance deepseek demo right fantastic we if you go to image you know of course we don't have any image as of now and this is fantastic you can create your own image guys you know if you want I think I will give you the link just go
and take the $5 for you guys uh the $5 free let me know if you need more credits uh I can figure out something if you're working on cool projects you know we can co-build it together using rank Okay. Uh and if you have any kind of question, thoughts or feedbacks right on on this, let me know. Happy to help and support. And if you like the video, please hit the like icon. If you haven't subscribed
the channel yet, please do subscribe the channel guys. That motivates me to create more such videos in near future. Right? If you have any questions, reach out to me through my social media channels. Find the information on channel banner and channel about us. That's all for this video, guys. Thank you so much for watching. See you in the next one.

===== END TRANSCRIPT =====

===== BEGIN TRANSCRIPT =====
Video ID: Es_a50LBuU8
Language: en
Source: auto-generated
Saved At: 2025-08-17T12:07:12.403334
Welcome to the deep dive. We're here to help you get informed quickly. Today, we're tackling something pretty crucial for anyone working in AI uh or machine learning. It's about picking the right cloud GPU platform. We've looked into two big ones, RunPod and Vast.AI. And well, they're built quite differently, you know. So, our mission here is to sort of unpack the unique vibe of each one. We want
to help you figure out which one makes sense for your projects, especially the really demanding stuff like uh distributed training jobs. Okay, so let's start with runpod. It often comes up for let's say serious multi-node training like if you need eight or more A100s or H100s. >> Exactly. Runpod really shines there. A big part of it is their instant cluster feature. It's um pretty powerful. Just a
few clicks or you can use their API and boom, you get a synchronized set of GPU pods all in the same region, same environment ready to go. And it has built-in support for the tools you'd use like PyTorch Lightning, Horovva, TensorFlow, Multiworker, or even cluster managers like Slurm and Ray. That really cuts down on the manual setup time, which honestly can be huge, right? That setup hassle is
real. And I saw mentions of the tech advantages, too. Something about the networking. >> Oh, absolutely. The networking is consistently solid. You're often looking at multi-00 gigabit backbones of the data centers. Sometimes you even get things like NVL link or NV switch for that super fast GPU toGPU talk >> which is vital for scaling complex models. Right. >> Precisely. That kind of bandwidth
means you get really good scaling efficiency often like over 90%. >> Wow. Okay. And speed isn't just the network. >> No. Speed in general is a big plus. They have this thing called flash boot. It gets your containers running in just a few seconds. So scaling up even to maybe 50 GPUs. It just feels less less intimidating. Plus, you get dedicated GPU access, >> meaning noisy neighbor issues. >>
Exactly. Noisy neighbors, no slowdowns from virtualization layers that you might see elsewhere. It means predictable performance, which is crucial for those really important jobs. >> And how does the pricing look? >> The pricing is pretty straightforward. An H180 GB might be around, say, $2.79 an hour. An A180 GBY maybe $119 an hour. And it's build per minute or even per second if you go
serverless. >> Per second. And a really key point, >> no charge for data egress. If you're moving lots of data out, that can save you a lot. >> That's a big one. What about reliability, predictability? >> Runpod is known for being super stable. They run in managed data centers. They have SLAs's, so you know, zero surprise shutdowns. Basically, their support is 24/7 and crucially, they actually
understand AI workloads. It's a good safety net. Oh, and they also do fractional GPU billing, which is nice if you have smaller tasks. >> Okay, that paints a clear picture for RunPod. Let's uh switch over to vast.ai then. This one seems to be the go-to if you're really watching the budget or maybe want to test on like older GPUs. >> Yeah, that's the core idea. Vast.ai is essentially a global
marketplace for GPUs. You find a huge variety of machines. There could be anything from consumer RTX 3080s, 4090s up to older pro cards like V100s. And because it's a marketplace, you can often find instances that are maybe 30 50% cheaper than standard platforms, >> especially the interruptible ones. >> Especially the interrupted ones. Yeah. You trade off some stability for even lower costs there.
>> Okay. Cheaper, more variety. But what are the trade-offs? Especially thinking about those bigger distributed jobs we talked about with RunPod. >> Well, the biggest difference is that vast.ai doesn't really have a native built-in cluster service like RunPod's instant cluster. So for distributed training, you're basically doing the setup manually. >> Meaning >> meaning you launch the individual
instances yourself. You handle the networking between them, share SSH keys, get your scripts coordinated. Their API and command line tools help for sure, but it's more hands-on. And you really have to watch the network locality and the provider specs. Your nodes could be hosted by different people, maybe even in different parts of the world. >> Ah, so latency could become a real issue. >> Huge
issue. Yeah. If nodes can't talk fast, your training slows right down. And what about getting data onto those nodes if they're all separate, >> right? That's another hurdle. Each instance is isolated, so you generally have to download your data set onto each machine individually, which if you're dealing with terabytes of data, well, it's annoying. >> Yeah, sounds cumbersome. >> It can be. Also,
startup times for your software images can vary. If the specific host you rented doesn't have your custom Docker image cached, it might take longer to pull down. >> And reliability. You mentioned interruptible instances. >> Yeah, reliability on vast.ai is just well, it's more unpredictable overall. Some hosts are great, totally rock solid. Others, you might find a node just drops out mid-training.
So, it's pretty crucial to use training frameworks that can handle restarts gracefully. Checkpointing is your friend, >> especially if you're going for those cheaper interruptible spots. >> Absolutely. You have to expect interruptions there. So boiling it down, RunPod seems tailored for the serious large-scale distributed training where that managed environment, the fast networking, the
consistency, it saves you real time and effort. >> Exactly. It's optimized for that. Vast.ai though, it's fantastic when your main goal is cheap compute. Great for maybe shorter experiments, jobs that aren't time critical or if you specifically want to test across a wide range of different GPU types. You get more flexibility, potentially much lower costs, but you're managing more of the stack
yourself. >> So to wrap up, both RunPod and Vast.AI clearly offer powerful GPU options. But as we've explored, they really cater to different needs, different priorities. >> That's right. This deep dive, I think, highlighted the key differences in their design, their pricing, and importantly, their reliability. The choice really comes down to what you value more. Is it that managed convenience and
guaranteed performance for big critical jobs? Or is it maximum budget flexibility and more hands-on control maybe for more experimental work? >> So the final thought for you listening is what does this mean for your next big project? How will understanding the strengths of RunPod versus vast.ai help you optimize your resources, your budget, and ultimately accelerate your discoveries? Hopefully
this deep dive gives you a clearer path forward.

===== END TRANSCRIPT =====

