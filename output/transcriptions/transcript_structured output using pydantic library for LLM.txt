===== BEGIN TRANSCRIPT =====
Video ID: u8tSzHb45MM
Language: en
Source: auto-generated
Saved At: 2025-08-17T11:15:36.318522
Hello everyone and welcome to the fourth part of Google agent development kit for beginners. In the previous video we looked at how we can use multimodel for example using openAI cloud for our models. So creating an agentic team of AI agents with different models and providers. In this particular video, we are going to look at something called structured outputs. And that is telling the large
language model what sort of format to give the output on, which is usually a JSON format. And that is really really useful when you want the large language model or your agentic AI team to return back a JSON instead of text. When you get the data back to your front-end application, for example, in a JSON format, you can compare the properties against your own code and you can execute certain
actions. And with that you are always going to be sure that you get the exact same kind of response on every single call to the agentic team. If you look at the documentation on the Google ADK site, you can see that when we are structuring data or we are using structured data, we have a couple of things here. We have the input schema, the output schema and the output key. The input schema is what
you will send to the agent as an input. The output schema is what the agent is going to generate for you. And the output key is similar to what we have discussed before. This is going to be the key of the output from a particular agent that can be accessed by other agents as well. So they can identify which output is from which particular agent. Having said that, when we are actually using
structured output, we can use piantic for creating a class. And once we have created the class, we can essentially use that class in both the input schema or the output schema or based on our current situation. Having said that, this is what we are going to build today. We're going to build a sequential agent or a team of sequential agents which will have two things. One will have problem analyzer
agent and then the advice generator agent. You can imagine this agentic AI team working for a consultation application like STNR which is really famous in Saudi Arabia for example where customers can actually connect with the right consultants for example a physician a psychologist a coach for example a physiootherapist and whatnot and imagine an agentic team that can ask the user what do they
want and then can suggest them the right kind of consultant and the questions to ask from that particular consultant. That is what we are going to build in this particular video. And by the way, if you didn't know, the SNR app was actually built by an amazing company named IMAX, which I am proud to be a partner of. And if you want to know more, you can find the link in the description of this
video. Again, having said that, the user input is going to reach first the problem analyzer, which is going to use a structured output class named problem analysis. Once it's done with that, then it's going to go to the next agent with this particular output problem analysis result as an input to the next agent. And then this particular agent will use this consultation response class for
structuring the output and then we'll give back a response in JSON. Well, enough talking. Let's actually get to the code. If you have not already cloned the repository, I would suggest you to find the link in the description of this video and then just clone the repository. And for this particular tutorial, you need to switch to a branch named for structured output. So if I do g branch, I can
already see the name because I already cloned it or checked it out. So I can do g check out for structured output. And now I'm on the right branch and you should be as well. Then you can just open this into your favorite editor. I'm going to use NVIM here. And then the first thing we want to do here is to create a folder. So we're going to create a folder called structured output. Then we are
going to copy some files. And if you don't have the ENV file, you can essentially look into the marketing campaign agent the first one. And you can see the ENV example here. You can just copy paste that and then add your variables for your Google API key, open AI key, etc. But I'm already going to go here and I'm going to quickly copy these and then paste them inside here. And if you want to look
at the initon file, this is how it looks. We just do from.imp import agent, which means we need to create a new file here called agent.pythyon. And inside here, we can start working. Now, as I said before, we need to use pientic here. So, we are going to go to requirements.txt and here we are going to add something called pientic and we're going to quickly save this. Let's also quickly install
this. So I'm going to do source venv bin activate and the command for windows is going to be a bit different but you can just Google it. Once I have my environment or virtual environment activated I can quickly say pip install r requirements.txt. So this is going to install pyic for me. Now I'm good. Now I can quickly go ahead to the agent file and start coding the agent. So I'm going to be copy
pasting a lot in this video. So feel free to stop the video, pause it and then copy and code it alongside. But you can always find the final code in the repository on the main branch. So I can go ahead now and start importing important things here. So we can say from enum we import enum as pi enum just so we don't have any conflicts. Then we can also do from typing and here we can say import.
We're going to import two things list and optional. Now we can import the agent. So here we can say from googleadk.tagents import lm agent and you can also just use the alias which is agent. I'm just going to use llm agent here. And then we are also going to have a sequential agent here as well. Now I can say from pientic import base model and field those are the two things that we need. Now we
are first going to create an enum for all the consultant types for example psychologist psychiatrist etc. So we can say class and we can say consultant type enum which is going to be a pi enum and then what I want to do here is I want to create a bunch of consultants here. So these are going to be the consultants that I have. We have psychologist, psychiatrist, therapist, nutritionist, personal
trainer, life coach, financial adviser, business coach, career coach and then a general helper which is going to be used as a fallback. Then we are also going to define the structured output for problem analysis for our first agent. So we can say class problem analysis and in here we are going to use the base model and this is going to have two things. We will have a consultant type which is going
to be an enum just like this or we can essentially just say this and then we'll have an identified issue and here we can say summary. This is going to be of type string and we are going to say this is going to be a field and we can quickly give a simple description here and we can use a description as such. So we can say a brief summary of the core issues identified from user's query. And of
course we need a comma here. So this looks good. Now we can format this and this is still looks weird. So I'm going to quickly try to format this. All right. Now we are going to create the instructions for this problem analyzer agent. So we are going to say problem analyzer instruction. And here we can have these instructions. And here we are saying you are an expert AI system that analyzes the
user's query to understand their core problem and recommend an appropriate type of consultant. Based on the user's query, identify the primary issues the user is facing. Then determine the most suitable consultant type. output a JSON object with the recommended consultant type and concise identified issue summary. We using the same kind of key properties that we are using in the schema. If you
skip it, even the agent is going to still analyze it because we are using essentially Pythonic for this one and we're going to pass that as a schema as well. But this makes our case even stronger. Now it says if the query is too vague or doesn't clarify to fit a specialist recommend general helper. So that's the fallback and focus on the main problem. Now we can start creating our agent. So here
we can say problem analyzer agent. We are going to have an LLM agent here. First of all we can give this a name problem analyzer or actually problem analyzer agent. Then we can give this a model here and the model is going to be Gemini 2.0 flash. Then we are going to give the instruction which is going to be the problem analyzer instructions. Then we are going to give this an output schema. So we
can say output schema is going to be problem analysis. This should be the same class that we have created right here. Finally, we say output key is going to be problem analysis result. So, let's make sure that this says just like this. I'm not sure why, but this doesn't seem to format it as I actually want it, but it's fine. Now, we can move to the next agent, which is essentially the responder
for us or as we named it advice generator agent. So, let's use that. So, we're going to say advice generator instructions. And here we're going to quickly start giving this instruction. And these are a bit long. So I'm going to quickly go through them but obviously feel free to pause, copy or just copy it from the final code. So here we are saying you're a helpful AI as detail that provides
initial guidance based on the recommended consultant type and identified user issues. So this is saying that you have to work on the previously given input. And here I'm saying based on the provided output problem analysis result. This is important. This has to be the same as this guy because when you start using these curly braces inside instructions, they act as state variables. So the agent
development kit is going to try to find this variable for sure. Now we are saying generate the following in a structured JSON format. Now we are saying suitability explanation right. So why the consultant that we are suggesting is the right consultant. So this property is going to have that message key questions to consider to ask the consultant. That's really helpful for obviously the customers
or the people who are looking for consultation. Then we also have initial actionable steps if there are any. And then we are saying for particular consultants what's going to be helpful when you reach out to them. For example, for psychologists and psychiatrists, therapists suggest things like consider journaling your feelings or look into mindfulness exercises. Right? So these are going to be
some suggestions but not suggestions that are professional. So these are still going to be with a disclaimer at the end. Now we have for nutritionist, personal trainer, financial adviser, etc. And then at the end we have a disclaimer. So we are saying always include the standard disclaimer. This is an AI generated guidance and not a substitute for professional advice. Please consult with a
qualified professional for your specific needs. Really important. Now that we have this, let's quickly generate our agent. So our advice generator agent looks something like this. It is also an LLM agent. It's called advice generator agent. We are using the same model Gemini 2.0 flash. We're providing the instruction just like this. And I think this should be instructions. We're going to make sure
that this one also says instructions exactly like this one. Problem analyzer instructions. Yep, that's the same. Then we are saying the output schema is going to be consultation response. If you look at that, we have not created that at the moment, but we are going to create it in just a bit. And then we are saying the output key is going to be final consultation response. Now when it comes to
consultation response, we are going to create it just like this. It's going to have the properties that we already have in the instructions but explained like this. So we should have a consultation type which is using the same enum. So it's going to be either a psychologist, psychiatrist etc. Then we have the identified issues summary. This is going to be the same as the problem analysis. So it's
going to contain the same value basically. Then we have the suitability explanation. This is what we are generating from the advice generator. So here you can see that we talked about the suitability explanation right here. Then we have the key questions to consider initial actionable steps and a disclaimer all of the things that we already talked about. And now that we have our agent created as
well we are going to convert the root agent or we are going to create the root agent that is going to be the sequential agent. So here we can say root agent equals sequential agent and in here we are going to say the name of this is going to be we can say structured consultation workflow or agent doesn't really matter. Then we can provide the sub aents just like this. So we need to provide both
the problem analyzer agent and the advice generator agent. And that's pretty much it. Now that we have all of this created, let's quickly save this and we're going to try to run this now. And we are going to hope that this runs the first time. So if I do ADK web, we can go to our browser. We can go to localhost 8,000. And if we go there, we should be able to find our structured output agent right
here. If I go here and if I say for example, I have a weak left foot when playing football. So if I send this, let's see what it says. Okay, we have an error. line 21. So we go back to our code and we go to line 21 which is right here. And I can see the issue here is the comma that we should not have. We can quickly save this. And I think that's also the reason it was not formatting our code as
well. So if I go all the way bottom. Yeah, now everything is formatted. So my bad. Now I can quickly go to ADK web and run this again. Let's quickly go back to the browser. I'm going to copy this. Refresh this. And now if I ask the question, now we have validation error for LLM agent. Let's have a look at what's wrong here. So I think we made a mistake about the properties. So let's quickly go
ahead and check this here. I can see I made the same mistake as before. This should be instruction and not instructions. We have the name, model, output schema, output key. All of those look okay. Then in here we have instruction as it should be. We don't really have the input here which is uh interesting. So we should actually have for the advice generator the input schema as well. So input
schema should be problem analysis just like this. And now let's quickly save this. We're going to quickly go ahead and run this again. Go to our browser. Copy this. And now I can ask the question. And now you can see that we got our first output which figured it out that the consultant type should be personal trainer and the identified issue is weak left foot in football requiring physical
training. Now that we have the response here, let's actually analyze it via the inspector. So here if I go to the response or actually if I go to the event here, we should be able to see the state delta here which essentially shows the response. So here we have the final consultation response. We've gotten the consultant type personal trainer. We've gotten identified issues. So we clamp foot in
football regarding physical training. Then we have the suitability explanation. Why was a personal trainer recommended? So here a personal trainer can assess your lower body strength and develop a plan to specifically target and improve your left foot's strength and stability. So it's actually explaining why this is suitable. Then we have key questions to consider. What are your specific football
related goals? Okay. And then initial actionable steps as well. Consider starting with some light exercises to improve ankle mobility. Now this is something again is being recommended by AI agent. That's why at the end we have this disclaimer that says this is an AI generated guidance obviously consult the professional. So this is really cool because this gives the customer initial steps to follow
but then also gives the application or our system the JSON that it can look at to find more information. For example, if my front- end application, my web application or my mobile application gets this response back then I can quickly immediately see okay what is the consultant type? This is a personal trainer. So I can make an API call to my backend to get the personal trainers that I can
recommend to my client or this user which is really really powerful because just by asking a question from an agent we now have a whole flow where a consultant can be recommended based on the user's query. This is super cool. And with that said of course we come to the end of this particular video and in the next video we are going to look at what is state how can we also run this code
programmatically so to say and not via ADK web. So we can create our own Python file and actually run the code or run our agent. That's going to be super cool. But before leaving you, I again want to remind you if you found this video useful, smash that like button as hard as you can. Share this video. That really helps the channel grow and also motivates me to create these tutorials for free for
you folks as well. And hopefully once this series is finished, I can also take requests from you folks on what I should be working on next. I've gotten a lot of responses for React, Nex.js, etc. Let me know in the comments if there's something that you are interested on and I can start looking into that as well. With that said, as always, happy coding.

===== END TRANSCRIPT =====

===== BEGIN TRANSCRIPT =====
Video ID: IWyUUrvmb2A
Language: en
Source: auto-generated
Saved At: 2025-08-17T11:15:37.564913
This video is a crash course on Llama Index. You may be saying, "Lama index has been around for a while, so why are you actually doing a video on Llama Index?" Now, if you look at the Llama Index website just a year ago, the tagline was turn your enterprise data into production ready LLM application. But if you look at the tagline now, it says redefine document workflows with AI agents. So they
have totally pivoted into AI agents and they have categorized the products into document passing, data extraction, knowledge management and agent framework and this is where all the open-source llama index framework resides. Let's start by looking into the product offering itself. We have document parsing. They've called it llama pass. It's used to transform unstructured data into LLM optimized
formats. Best Gen AI native passing platform built specifically to transform complex documents with tables, charts, images, flow diagrams into clean data for LLM application. It basically transforms your unstructured data into LLM optimized format. You can take any unstructured data and convert it into a format that the LLM likes. Here's an example they have given for passing. So if you have the
input document which is a PDF document let's say and it has all the charts the output of llama pass would be something like this where all the data is pass and you have all the data in tableau format or textual format it doesn't matter whether it's a financial report or research paper or technical manuals llama pass streamlines your document workflow enabling you to focus on leveraging your data
and it supports lot of files in right from PDF to XML to even epop so you can crunch any type of file into an LLM. And you can also choose what sort of speed that you want. So if you want the fast mode, it excels at text and tables. But if you want the premium mode, it can handle any document type giving the most accurate and comprehensive results. You can configure to have a output format in say
JSON or a markdown. And you can configure this in the llama pass. So that was llama pass. The next possibility is data extraction. They've called it llama extract and it's effortless structure data extraction. Basically, you can define a schema and extract structured data from any document, be it invoice, contract, claims, PDFs with high accuracy. Let's look at how it works. You define a schema
like it's defined here and you pass the original document and at the output you get the extracted result in a structured output say a JSON format. So, it's as simple as that. That's what they are saying again. You upload the documents, define the schema or the prompt and you run and retrieve structured output in JSON format and integrate directly in your applications. For example, in the finance
case, you you can extract fields from invoices, receipts and financial statements. In the legal case, you can summarize and extract key entities like contacts, legal filings and in healthcare you can pull data from say clinical notes or discharge summaries. And in case of insurance, you can extract fields like claims from downstream workflows, so on and so forth. And the third one that's on offer
is the knowledge management and they're calling it the llama cloud. And they're saying you can get accurate and secure knowledge management of our AI agents. So if you want to make any unstructured data LLM ready, then you can go for Lama Cloud. It's it's like you have everything under one roof that's provided by Llama index itself. So if you click on get started, you'll get this page to sign in.
So if I sign in with my Google account and I'm now signed into Llama cloud. We can see that there's pass here, this extract here, this index here. And you can create a pipeline with all these. Let's play around with pass to begin with. The recommended setting is this. I'm just going to leave it like that. And on the right, we can see this add a file for passing. We can either upload or we can give
the file URL. I'm just going to drag and drop a file which is about the Apple's latest 144 filing. It's just a two-page document. I'm going to drag and drop the file here. It's got uploaded. I'm not going to change any of the details and I'm just going to click on pass. So once it's finished parsing the document, we can see the different formats that we want the outputs to be. I can click on text
or JSON or whatever. So if I'm happy with the JSON format, I can click here to download it and I can use the downloaded format for any other purpose. The Llama cloud is for someone who is less technically inclined who cannot really code much. But if you want to actually get your hands dirty by coding, then they are offering agent framework. So agent framework is the one to orchestrate and deploy
multi- aent applications over the data. Let's have a look at that production ready framework for LLM agents. So let's look at the GitHub page now. So the repository is MIT license which is open source. And if you want to get started, we can either run pip install llama index to install everything together or we can install each of them separately. For example, we could do pip install llama index
core, pip install llama index, llm's openai and llm's replicate. So this is the documentation and in the documentation they have got introduction, use cases, getting started, llama cloud, community and related projects. And on the left side we could see that there's installation step installation and setup and then how to read this docs. We will do some quick hands on by installing and setting it
up. But let's look into how to read this documents. We need to make sure Python is installed. Otherwise if you're working with JavaScript it's going to be a TypeScript package. So once we install the library then we can move on to writing the five lines in demo. That's the starter tutorial. So, so I will walk you through the starter tutorial towards the end of the video. But let's get back to what
else we need to know to get started. Then we need to learn about the highle concepts. So let's look into the highle concepts. So we need to know what the LLMs are and we need to know agentic application. When an LLM is used within an application, it is often used to make decisions, take actions or interact with the world. This is the core definition of an agentic application. We also need to know
what is LLM augmentation. You know we can augment the LLMs with arbitrary call functions or we can augment with memory or could be dynamic prompts. We also need to know about prompt chaining. So if we have several LLM calls and the output of one LM called being used as input to the next one then it's called prompt chaining. Then there's something called routing. So routing is used to route the
application to the next appropriate step or state in the application. Then there's parallelism. So the application can perform multiple steps in parallel. And then there's orchestration where you know that we can have hierarchical structure of LLMs used to orchestrate low-level actions and LLMs. And then there's reflection. And the LLM can also be used to reflect and validate outputs of previous
steps or LM calls. So that's reflection. So we have done we have implemented a reflection agent in our past video. So you can have a look at that as well. And we need to know what agents are in general. So an agent is a piece of software that semi-automatically performs tasks by combining agent LLMs with other tools and memory. So we have built quite a few agents in our previous videos. You can
have a look at that. And then there's retrieval augmented generation. Again we have done videos on introduction to retrieval augmented generation. You can have a look at that and come back to this. So the use cases could be for building agent or it could be for building a workflow. So a workflow is nothing but orchestration of a sequence of steps and LLM calls and structured data extraction. This
is where you use pyantic schemas to define the structure of the data and you simply extract data and then you feed into any LLM to fill the missing pieces in a type- safe way. And then this query engine is an end to-end flow that allows you to ask questions over your data. So your data can be anything. You post it in a interactive way so that you can ask questions about whatever is there in your
data and then obviously you can build chat engines which is an end to-end flow where you can have conversation with your data and you get answers for the specific questions you ask. So these are the different use cases and these are the concepts that we need to be familiar with. With that introduction, let's move on to the starter example. They've given example of how we can go about building a
simple agent using llama index and how we can add rag capabilities to the code. So to get started, I've created a cond environment called llama index. So I'm just going to activate that llama index start. I've activated the virtual environment. And one of the next things that we need to do is set the OpenAI API key. So I'm going to say OpenAI API key is set your OpenAI API key here. Let's start
the Jupiter notebook by typing Jupiter lab. So I've launched the Jupita notebook and I'm saying pip install llama index and I'm also installing OpenAI because we're going to be using OpenAI models. Particularly I found this version going very well with LMA index. So I fixed the version to 1.83.0 zero and the installation went through fine because we sent the environment variable for the OpenAI API
key. I'm getting the OpenAI API key through OS.en. We're going to be building a basic agent and we're going to be introducing context to it. We finally we're going to just do a simple rack pipeline. I've imported the needed classes. The simple agent we're going to be building is a function agent. So it's going to take the multiply function as a tool and the LLM is going to be the GPD40 mini model
and the system prompt is you're a helpful assistant that can multiply two numbers and to run the agent we do agent.run it's and and ask what is a * b and it's giving a response saying that the result of a * b is this. So that's a basic agent we can create with llama index. So if you want to introduce context to it, we can say context is context of agent. Now we will notice that the agent has
memory. So if I run my name is AIS ctx is context. And if I again go back and ask what is my name, we will see that it has remembered my name as AI bytes. Let's see what it does. So if I just print the response, we can see that the name is AI bytes. So that's how we can introduce context to it. And then we can also build rack pipelines. I'm just going to build a simple rack pipeline. For running a
simple rack pipeline, what we're going to do is we're going to create a directory called data. And inside that we're going to download polar essays which are quite famous ones. And we can see that inside data now we have the polar essays. Now we can quickly do some rag on the data. So to do rag first we need to create object documents. So documents is simple directory reader of the data the folder
that we just created and then say load data to index we're going to say vector vector still index dot from documents and we're going to pass the documents that we just loaded and then the query engine is of course going to be the index do as query engine. So those are the three main lines. So we're loading the document. We are creating the index using vector store index from the document and we're
saying the index is now as a query engine. So if I just run that, it's going to take a while because it's doing all those stuff. So that ran fine. We can now interact with this document. But first we need to create our agent. So in agent is function agent and the tools is obviously search documents. We haven't created this tool. So let's create this tool by saying define search documents as which
what takes it takes a query uh which is a string and it returns a string as well and inside that we will say response is avoid query engine dot async query of the query and return the response as a string. So this is the function obviously because we are using async query uh we are going to also say async define search documents. So that tool is now defined and we are going to be using the open
AAI model for this which is going to be GPD4 mini and the system prompt is you are a helpful assistant that can search through documents to answer questions. So that should do for the system prompt. We have created the agent. Let's run that. Yeah, that agent is created. Now if we want to work with it, we can just say response is avoid agent.r run and we'll give the query that we need to pass which
is what did the author do in college. Let's print the response and see what the answer we get. In college the author applied to art school, attended classes in fundamental subjects like drawing, color and design and participated in the painting department at the academia. Also wrote essays on various topics. Worked on spam filters. did some painting, hosted dinner for friends and eventually
started his own investment firm with collaborators. So that's definitely Paul Graham. So it has pulled data from the document that we just added to the data folder. So that's a simple rack pipeline we have created. One thing to note is that everything is agent based. Now to begin with even to build a simple pipeline, we first creating an agent and then that agent is responding to your query even
for a simple rack pipeline. So that's the basic getting started hands-on. And there's one more thing that I wanted to show you which is the llama hub. This is where all the integrations from a contributor or a collaborator is listed. For example, if you want to work with say multimodel model and your multimodel model is there in hugging face, then you can use this. Basically, you can do pip
install llama index multimodel llm's hugging face and you can set your hugging face API key. You can straight away import the hugging face multimodel class from llama index and you can create hugging face multimodel. model name and you can pass your own model and you can use that because the model is multimodel you can you can pass an image to as a input and say image document and you can describe
the image in detail and you get the response about the image. So that's super cool and we have several contributions here. If I select all integrations, you can see the different integrations that are available. So you can even build your own integration for your own system and you can contribute because llama index is open source. So just to summarize what we saw, we started with looking into
llama pass which is for document passing and then we quickly looked into llama extract which is for extracting the data by defining a schema. And then we also saw about Llama cloud. It's for people who are less technical and who want to manage their data in the cloud in Llama's own native cloud. And we also saw about agent framework. Particularly we saw how we can build your own workflow. We
quickly saw how we can build a rack pipeline. Not to forget we also have llama hub. If you want to do some contribution to llama index and build your own integration into llama index then llama hub is the place where you want to contribute. So, I hope it's useful for you to get started quickly with Llama Index and I will see you in my next video.

===== END TRANSCRIPT =====

===== BEGIN TRANSCRIPT =====
Video ID: 4mye7oi0CXk
Language: en
Source: auto-generated
Saved At: 2025-08-17T11:15:38.683207
Switching from one LM provider such as OpenAI to another like Google can be a technical nightmare if you are too dependent on the particular SDK that the provider is using. So one way to fight this is to create internal abstractions over their API or another one is to use a library such as white LM that abstracts away the differences between the different LM providers. Hey everyone, my name is
Vanin and in this video we're going to have a look at white Loom, the library that allows you to connect to multiple LM providers. This library is very different compared to something like open router that allows you to call an API and get the response from different providers. White M is a library or an SDK that you integrate within your software stack and you use it essentially as a wo library.
It gives you a complete abstracted interface towards pretty much every available LM providers including OA. So if you want to work with local AOS and switch from time to time to some of the providers then you can do that with this library. You can also get a completely unified way to do function calling or tool calling and get structured outputs. In this video, I'm going to show you how you can
set up the library, how you can call different OM providers, how you can get structured output in the way of using pedantic objects, and then we're going to do a quick demo on how you can do tool calling. Let's get started. If you want to become better AI engineer, you can go and subscribe to ML Expert Pro. There you can find a complete AI boot camp that starts from the basics of learning Python
classical machine learning and statistics. Then going through a complete deployment of a machine learning pipeline. Then you're going to go and learn about how you can prompt them, how you can call tools, functions, how you can build complete ARC applications and CAS. Also, you're going to be learning how you can build aic systems, eight uh GitHub projects that are also available for MLX for Pro
subscribers. So, if you want to become a better AI engineer today, go and subscribe to MX for Pro. Thank you. I have a Jupyter notebook opened in my cursor instance. Here you can see the main dependencies that we are going to be using the light om library pedantic and numpy doc. This one is going to be used to parse the dock strings from the tool that we're going to be defining later on. Then you
can see the dependencies that I'm going to be importing. And uh the thing here is that I'm going to be showing you how you can call both OpenAI and uh Google's Gemini models. So uh here I'm going to be using to my keys for OpenAI key and Gemini API key. Here will be the test prompt defined what is tarning in one sentence and I'm going to be using this list of dictionary with a simple prompt that
we have defined. This is very similar to what you might have within your OpenAI SDK. So in order to do a completion test I'm going to be using the completion function from white LM. Here you're going to be providing your model name. Uh this by default uh goes through OpenAI models. As you can see here, I'm specifying GPT 4.1 mini and I'm passing the messages. So this will give you a response. As
you can see, this is very similar to using the OpenAI SDK. If you go through the response itself, you can see that it is typed as a model response. And here you can see the model that you have used to do the completion, the different choices of the completions. If you wanted to do more than one completion, that is also available. And here you can see the message itself. This is the response from
the model. We're going to see it in a bit. And then you have a detailed breakdown of the usage, completion tokens, prompt tokens, and the number of total tokens. Also, you can see here if you have some additional details such as cash tokens. Uh, as you can uh pro, as you probably already know, pretty much all of the LM providers nowadays allow you to cash your prompts and this save on your uh
views essentially. So looking at the response usage, you can uh get a detailed look over here or you can just get the message content from the response. And here you can see the response from Gypy uh 4.1 mini. Deep learning is a subset of machine learning that uses multi-layer neural networks to automatically warn hierarchical representations from large amounts of data. Okay, pretty good response.
Then uh Google have released their stable Gemini 2.5 models in the name of pro and flush. Uh here you can see that I'm calling the 2.5 flush model and we have a sort of name space or a prefix for these models with the name of Gemini. Right here I'm passing in the same messages and I'm adding an additional parameters since this is a thinking or reasoning model. So for this example, I don't want to
have any thinking uh associated with this request and you can see that the interface is exactly the same which is the powerful thing about this library and you can go through the response itself. It is pretty much the same and you can see that we are getting this response. Deep learning is a sub field of machine learning that uses neural networks with multiple layers to learn complex patterns from
data. Okay, I would say slightly better response here from uh 2.5 flush. But let's continue with the structured output example. One very important thing that I wanted to mention is that you can use such library such as white when you're doing uh internal evaluations of the different models or you can use this as a fallback. So for example, if OpenAI's API is down or it is too slow for some of your
requests, you can use library such as white AOM to provide fallback for the requests and you can essentially fuel in the requests that are failing with uh for example Gemini or anthropic models until OpenAI models are back online. So the next thing that is pretty common practice to use when working with LMS is the structured output. So in this case I would like to get a response in the form of
this uh pedantic model. I want to do some sentiment classification with the possible options of negative, neutral or positive. And I want the model to provide me with the reasoning of why this sentiment has been chosen. So here is the very simple prompt that I'm going to be using. Classify the text sentiment into one of negative, neutral or positive. Give your reasoning in the reasoning field.
This is a common practice uh in the way that I'm going to be using this reasoning field as a sort of a thinking tool if you will. So next here you're going to be passing in the text. I'm very happy to say that AI has taken my job for good. Well, uh we'll see uh what the sentiment of this is going to be and I'm going to be doing pretty much exactly the same completion and I'm going to be passing in
the response format with this pedantic uh model that we have created. This took a bit more time uh to complete but if you look at the response you see that it is just a JSON string properly formatted and you can get your sentiment classification model itself with model validate JSON which is coming from the pedantic library passing in the JSON content and you can see that you got the sentiment
classification model. So here you can see the reasoning. The text explicit and the sentiment classified as positive. The text explicitly states that I'm very happy to say that AI has taken my job for good. The phrase very happy quasify indicates a positive emotion and for good implies a beneficial and permanent change reinforcing the positive sentiment. Okay, so probably the model didn't get the
irony here, but uh let's say that this is good enough for us as of now. So the next more important thing than structured outputs is actually tool calling. Modern AOM are more than not used in workflows and agentic systems. So in order to work with those you are going to be providing a different tools or set of functions that your models can work with. And in this case I have created this very
simple function. Estimate house price. You're going to be passing three different parameters. The square meters of the house the number of bedrooms and whether or not this is inexpensive location which can be subjective. And we're going to be using the Loom in order to uh provide this uh parameter for us. So you can see that I have a very detailed uh description of what this function does and what
the parameters are. This is going to be useful to the M uh to call this function properly. And the implementation is a very dummy one. I have just multiplied the square meters, number of bedrooms and whether or not this is an expensive location. So how do you use white to call these two? First thing that you need to do is to call this function to dict and this will create a list of tools for us.
Then you're going to be adding the prompt. This is going to be provided by the user. I'm looking at a three-bedroom house at San Jose. It's about 250 square m. How much should I get it for? And then you're going to be adding a prompt here to the messages. And the important thing here is that you're going to be specifying the list of tools and the tool choice is going to be set to auto. So what the
tools look like, you can see here that uh essentially you get the description, get house price estimate in USD square meters. This is an integer and square meters of the house, number of bedrooms. Uh you can see that those are taken here from these parameters and then you get the expensive location type boolean. I'm not really sure why it didn't get the maybe because I did add these semicolons.
Not really sure but it didn't got correctly the description of this parameter. So this is what the OM is going to get uh for the description of this function. And if you call the with this, you get essentially this two call which allows you to call the function with the arguments of expensive location equal to true, number of bedrooms equal to three and square meters equal to 250 and the name of
the function that the wants you to call is the estimate house price. So this looks uh pretty good. And in order to do the two calling itself, you're going to get the messages added to the uh list of messages that we have. Then we're going to get the two calls which is going to be uh this uh argument here and then the available functions. We're going to be creating this dictionary which uh
currently has only just our single function. So for the actual tool call, I'm going to be iterating over the two calls. Since the LOM can give you a list of two calls that you need to call and even though in our case this is just one, I'm going to be using this loop to go through the available functions. Then you're going to get the function name from here, the available function. So you're going
to essentially match the function name with the available functions. uh then you're going to get a function arguments from JSON. This is going to take this these arguments JSON right here. And I'm going to be using the double star prepend to the function arguments in order to call the function uh to for the response. Then I'm going to be adding a message with the to call ID, the tool, the name of
the function, and then the response. And to look at the complete response that the final OM call is going to get, uh we're going to see that this is the tool call ID, which should match this one right here. Then this is the row of the tool, the function, and this is the final response. Note that you need to return this response as a string otherwise the library is going to be having issues with
parsing that even though this is an integer. Uh this is a good place to convert everything to a string. And then uh I'm going to be creating this final call to the model. And this is the actual final response that you are going to be giving to your users. For a three-bedroom house of 250 square meters in San Jose, you should expect to pay around $650,000 US. So, this used the two and then based on
the two response, it has provided this response for us. So, this is it for this video. We've seen the white library in action and how you can use it in order to call different providers. For example, you can use OpenAI, Google, Antropic, Oama, and other providers in order to get unified or abstracted interface to all of the available LM providers. We've seen how you can get structured outputs and
how you can do tool calling or function calling with the different LM providers as well. We've seen that the library integrates very well with pedantic and you can use it in order to get a unified experience when calling different OM. This will be very useful when you're doing AOM evaluations or falling back to different providers once a particular one is offline. So, thank you for watching guys.
Please like, share, and subscribe. Also, join the Discord channel that I'm going to link down into the description of this video. Also go and subscribe to MXert Pro to become a better AI engineer today. And I'll see you in the next one.

===== END TRANSCRIPT =====

===== BEGIN TRANSCRIPT =====
Video ID: Q6KU7exTEhY
Language: en
Source: auto-generated
Saved At: 2025-08-17T11:15:39.883611
Welcome to developer service YouTube channel. On today's episode, we're going to be working on a project that demonstrates how you can use the mistrial API in JSON mode to convert a CSV data into a structured JSON object that is also validated as a pyentic model. It also includes robust error handling and retry logic to ensure valid output. So what are the main features of this example project?
Read CSV data from a file. It uses the MR API to convert the CSV to a JSON array of objects. Validates the output using a pedentic model which we call person. Retries with improved prompts if the validation fails the pentic validation and infers or skip rows with missing invalid data. So why why is this important? It is important because more and more we're using AI to process data and most of the
times we're using the AI to process the unstructured data and that needs to be further down the line processed by an API or another system. So it's always good to make sure that that unstructured data gets turned into a structured data and that can be done by the AI but it can be helped by having the proper validations with the pantic. So as a requirement for the the this project, we're going to
need a MR API key that you can get from uh from MAI and that's the only requirement besides of course having the MRI and the pyic python packages uh also installed in your system. So let's now take a look at the code that we're going to use for the example project. We're going to be importing OS JSON in pyate the business model and the validation error. And of course from M we're going to be
importing. We start by defining our padding tech model for validation. In this case, we're defining a class person uh which is a name, age and email name string, age and integer and email string that of course imports from base model from pent. Then we define a function to call the mal API in JSON mode. Uh so we are retrieing the API key from the environmental variables. If that's not present, we
raise a runtime error. We're using the MSAL large model. We create an instance of the client from the MR with the API key. Uh we define a set of messages that's going to be using a system message and also of course the user message. uh we create a just response and that is the output of the client chat complete and we're going to be passing in the model the messages and we're saying that the
response format and this is the important part that we wanted the type to be just an object and of course we're returning from this function the the response from the AI that is part of the output here we are reading the CSV input from a file in this case we have an example incomplete CSV which is a CSV that contains some missing records So we can see the AI and pentic validations in action. Uh we
are using the open encoding UTF8 in read only mode and we're storing the the contents of the CSV file in this CSV input that we're just reading from the file and stripping all the the whites space characters. And to show you the example, this is the example of the CSV. We have Alice with 30 in an email, Bob with a missing gauge, Charlie with a missing email, and also Diana with a complete example.
We then define our initial prompt. So we're defining the model JSON scheme of the the person of the the patented model that we just defined before. So they're constructing the prompt. We say the follow CSV data. We wanted to return a JSON array of objects with the fields matching the schema of the pitic model. the CSV we pass as the input and we also structure the example output based on the on
the panic model also with the name age and email and then we're just logging the the execution of that to the to the console and we're calling the the function that we have defined previously with a col with JSON mode and we're passing in the the prompt and in this case we're just passing in a user prompt the system prompt we keeping it as empty with the response of AI we're executing our
validation in retry loop. Uh so from the response we're trying to load that into a JSON object and then we're trying to process if we can map that into a person object uh or a list of person items. And if that is true then we're going to print the the output of that that array of person. Uh and we're also going to be seeing if there's some records that have been skipped that are not able to
construct as a person object. uh if we are not be able to transform this response into JSON object that means the parent validation field uh and in that case we are going to be entering our retry loop. So our retry loop defines which attemp attemp two of course because the first one is the attempt that we just did maximum attempt is going to be then 10 and we're going to be storing the last
response and the last error and of course while our current attempt is below the maximum attempt. So we're going to be trying this retry loop and let's now take a look at the improved messages for the retry. So we're defining an assist message. We're defining that AI is a data including set agent that the job is to convert the JSON data into a JSON object of that schema even if there is data is
missing or invalid to infer reasonable value to complete it and to also return a valid JSON. As for the improved user part, we're saying that kind of our CSV data to return again array of object of that schema passing in the CSV but giving more clear instructions. We're saying for each row make sure that the object that is created corresponds to that schema. If the field is missing to infer a
value, put an app row. Ensure the output is a valid JSON with no extra text. And important for the retail of the AI that it will use the last error and response to determine how to fix the error. And we're passing in those uh as inputs for the the prompt. And again, we're stating the example output to state exactly the JSON to be formatted. And we're logging in the execution to the to the console.
We're calling again the MR JSON mode API, our function that calls the API. We're passing in our improved pro prompt. And we're also passing in our system messages. And again, as before, we're trying to convert a simple data into a JSON object. Uh passing in the the resulting JSON object as a person model. And if that's true, we're printing it to the console. If there's any skip, we print that. Uh
if it's enable again to convert this into a JSON object, then we're going to be logging that error. We are going to be increasing the attempts and logging the last errors and the last response and entering again the the retail loop until we get an output. Let's now run the code. It's going to start by trying the the first attempt which has the simpler prompt. As you can see, it return uh a
response that is missing the fields. In this case, the Bob's age is new and the email for Charlie is also new. And we have the validation error from presenting stating that that one validation error for person. this case the h saying it's should be a valid integer because it's missing it's null in this case and so now that it's trying to do the second attempt which input from the system message it
has written again the response and as you can see this time we have the validated people both Alice Bob and Charlie so it processed all the records and also Diana and as you can see for Bob it in proper value in this case zero because it is does not have actual value for the email it has filled in empty unlike uh the new value that was filled in before and so that allows to process all the records
and we have the validated records. uh depending on the prompt and depending on the complexity of the input uh it can take anywhere between two to five retries. At least in my testing that was the that was the case. But this is how you can use pyic to help validate the outputs of the AI and make sure that in structure proper structure JSON format especially if you need to use the output to be
processed by an API or for further processing downstream in the pipeline. Uh as a final note just to mention that miss in this case besides the JSON mode that that we're using and we're seeing the missile documentation besides the JSON mode that we just saw that we can use the chat complete and passing in a response format with the type JSON object actually allows for custom structure outputs
where it we can define a pentic data model just like we pass like we define and we can use the chat p to pass u the message or the prompt into that particular format by passing as a response format that pentic model. Um this is useful if you have some sort of a structure that we want it to be parsed as the that structural data but in my example I kept a bit more generic because you can use this
JSON mode with several other different APIs from open AI uh to code. So it's a more generic approach to using pentic for validation. But just as a note that ming AI in concrete has a parse that can receive an actually pentic model. Uh like this video if you like this content. Subscribe for more content about Python, Django and AI. And until next time, happy coding.

===== END TRANSCRIPT =====

